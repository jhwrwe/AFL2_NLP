{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05b5a1f2-151c-4640-bcd3-0c7891e0057a",
   "metadata": {},
   "source": [
    "# 1. Background Problem (20%)\n",
    "Language modeling is a fundamental task in Natural Language Processing (NLP), used in various applications like predictive typing, text generation, and spelling correction. For this project, I chose the Sci-Fi Stories Text Corpus available on Kaggle. Sci-Fi literature is linguistically rich and imaginative, often pushing boundaries of vocabulary and structure. Modeling such text is both challenging and rewarding, and it provides an exciting opportunity to explore how well statistical language models and autocorrect systems can handle complex and creative writing. Recent research has demonstrated that large language models can generalize to a wide variety of tasks, including creative text generation and spelling correction, even in few-shot settings. Studies have also shown that while language models are capable of producing creative writing, they face unique challenges in maintaining coherence and handling the imaginative language found in genres like science fiction. Furthermore, advances in spelling correction techniques have highlighted the importance of robust language modeling for correcting errors in creative and domain-specific texts\n",
    "\n",
    "References:\n",
    "* Brown, T. B., et al. (2020). Language Models are Few-Shot Learners.\n",
    "* Clark, E., et al. (2021). The Effectiveness of Language Models in Generating Creative Writing.\n",
    "* Zhang, Z., et al. (2022). A Survey on Spelling Correction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4472924f",
   "metadata": {},
   "source": [
    "# 2. Resource\n",
    "\n",
    "We used the following dataset found from kaggle:\n",
    "\n",
    "Sci-Fi Stories Text Corpus by Jannes Klaas: \n",
    "- https://www.kaggle.com/datasets/jannesklaas/scifi-stories-text-corpus\n",
    "\n",
    "The dataset contains a collection of sci-fi short stories in plain text, which provides an ideal source for both syntactic and lexical modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0eaaf9",
   "metadata": {},
   "source": [
    "# 3. Methods (10%)\n",
    "## We applied the following methods:\n",
    "\n",
    "- Preprocessing:\n",
    "    * Lowercasing all text\n",
    "    * Removing punctuation\n",
    "    * Tokenizing into words\n",
    "\n",
    "- Model Building:\n",
    "    * Bigram Language Model (word-based)\n",
    "    * Trigram Language Model\n",
    "\n",
    "- Advanced Method:\n",
    "    * Autocorrect using edit distance and bigram probability re-ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8e77a7",
   "metadata": {},
   "source": [
    "## 4. Model Implementation Code (50%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4623b240-3c74-474e-b2de-b0b703944ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading corpus from /Users/stevgo/Downloads/corpus.txt...\n",
      "Preprocessing corpus...\n",
      "Total words before cleaning: 31924829\n",
      "Total words after cleaning: 26330559\n",
      "Building word pairs and triples...\n",
      "Word pairs and triples built.\n",
      "Preprocessing complete.\n",
      "Corpus loaded and processed in 75.98 seconds\n",
      "Vocabulary size: 303305 words\n",
      "Bigram pairs: 5117856\n",
      "Trigram patterns: 14801024\n",
      "Autocorrecting word: rocx\n",
      "Finding candidate words...\n",
      "Found 348 candidate words.\n",
      "Scoring candidate words...\n",
      "Candidate words scored.\n",
      "Top 3 suggestions: [('rock', 0.3), ('rocl', 0.3), ('roch', 0.3)]\n",
      "Autocorrect suggestions for 'rocx': ['rock', 'rocl', 'roch']\n",
      "Autocomplete suggestions for: cute\n",
      "Finding autocomplete candidates...\n",
      "Found 142 autocomplete candidates.\n",
      "Top 5 autocomplete suggestions: [('little', 58), ('and', 19), ('as', 18), ('she', 8), ('i', 6)]\n",
      "Autocomplete suggestions for 'cute': ['little', 'and', 'as', 'she', 'i']\n",
      "Predicting next word for: lovely\n",
      "Not enough words to predict next word. Provide at least two words.\n",
      "Next word suggestions for 'lovely': []\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd # Added to match original notebook\n",
    "from collections import Counter, defaultdict\n",
    "import heapq\n",
    "from typing import List, Dict, Tuple, Set\n",
    "import os\n",
    "import time\n",
    "\n",
    "class SciFiWritingAssistant:\n",
    "    def __init__(self, corpus_file_path: str):\n",
    "        \"\"\"\n",
    "        Initialize the SciFi Writing Assistant with a corpus file.\n",
    "        \n",
    "        Args:\n",
    "            corpus_file_path: Path to the corpus text file\n",
    "        \"\"\"\n",
    "        self.corpus_file_path = corpus_file_path\n",
    "        self.word_freq = Counter()  # For autocorrect\n",
    "        self.vocab = set()  # All known words\n",
    "        self.word_pairs = defaultdict(Counter)  # For autocomplete\n",
    "        self.word_triples = defaultdict(lambda: defaultdict(Counter))  # For next word prediction\n",
    "        \n",
    "        self.load_and_preprocess_corpus()\n",
    "        \n",
    "    #################################\n",
    "    # Corpus Loading and Preprocessing\n",
    "    #################################\n",
    "    \n",
    "    def load_and_preprocess_corpus(self):\n",
    "        \"\"\"Load the corpus from file and preprocess it.\"\"\"\n",
    "        print(f\"Loading corpus from {self.corpus_file_path}...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            with open(self.corpus_file_path, 'r', encoding='utf-8') as file:\n",
    "                corpus_text = file.read()\n",
    "                \n",
    "            # Preprocess the loaded text\n",
    "            self._preprocess_text(corpus_text)\n",
    "            \n",
    "            self._print_corpus_stats(start_time)\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Could not find corpus file at {self.corpus_file_path}\")\n",
    "            self._load_minimal_corpus()\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading corpus: {str(e)}\")\n",
    "            self._load_minimal_corpus()\n",
    "    \n",
    "    def _load_minimal_corpus(self):\n",
    "        \"\"\"Load a minimal corpus as fallback.\"\"\"\n",
    "        print(\"Using a minimal default corpus instead.\")\n",
    "        minimal_corpus = \"\"\"\n",
    "        science fiction space robot alien technology future\n",
    "        i am going to the planet mars\n",
    "        i am not sure about this mission\n",
    "        i am ready for the journey\n",
    "        probably the best solution\n",
    "        probably we should try again\n",
    "        hello there my friend\n",
    "        hello to everyone here\n",
    "        brother and sister went home\n",
    "        the sister was happy\n",
    "        \"\"\"\n",
    "        self._preprocess_text(minimal_corpus)\n",
    "    \n",
    "    def _print_corpus_stats(self, start_time):\n",
    "        \"\"\"Print statistics about the loaded corpus.\"\"\"\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Corpus loaded and processed in {elapsed_time:.2f} seconds\")\n",
    "        print(f\"Vocabulary size: {len(self.vocab)} words\")\n",
    "        print(f\"Bigram pairs: {sum(len(v) for v in self.word_pairs.values())}\")\n",
    "        \n",
    "        # Calculate trigram count\n",
    "        trigram_count = 0\n",
    "        for key1 in self.word_triples:\n",
    "            for key2 in self.word_triples[key1]:\n",
    "                trigram_count += len(self.word_triples[key1][key2])\n",
    "        \n",
    "        print(f\"Trigram patterns: {trigram_count}\")\n",
    "    \n",
    "    def _preprocess_text(self, text: str):\n",
    "        \"\"\"Preprocess the corpus text to build vocabulary and word frequency.\"\"\"\n",
    "        print(\"Preprocessing corpus...\")\n",
    "        \n",
    "        # Clean and split the text\n",
    "        clean_words = self._clean_text(text)\n",
    "        \n",
    "        # Build vocabulary and word frequency\n",
    "        self.word_freq = Counter(clean_words)\n",
    "        self.vocab = set(clean_words)\n",
    "        \n",
    "        # Build word pairs and triples\n",
    "        self._build_language_models(clean_words)\n",
    "        \n",
    "        print(\"Preprocessing complete.\")\n",
    "    \n",
    "    def _clean_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Clean text and split into words.\"\"\"\n",
    "        # Convert to lowercase\n",
    "        cleaned_text = text.lower()\n",
    "        \n",
    "        # Add spaces around punctuation (except hyphens and apostrophes in words)\n",
    "        for p in set(string.punctuation) - {'-', \"'\"}:\n",
    "            cleaned_text = cleaned_text.replace(p, f' {p} ')\n",
    "        \n",
    "        # Fix joined words by adding spaces before capital letters in the middle of words\n",
    "        cleaned_text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', cleaned_text)\n",
    "        \n",
    "        # Split text by whitespace\n",
    "        words = cleaned_text.split()\n",
    "        \n",
    "        print(f\"Total words before cleaning: {len(words)}\")\n",
    "        \n",
    "        # Remove words with special characters and numbers only\n",
    "        clean_words = []\n",
    "        for word in words:\n",
    "            # Keep hyphenated words and contractions intact\n",
    "            word = word.strip(\"-'\")\n",
    "            # Skip empty words, numbers, and special character sequences\n",
    "            if word and not word.isdigit() and not re.match(r'^[#]+$', word) and not re.match(r'^[^\\w\\s]+$', word):\n",
    "                # Additional check for joined words without spaces\n",
    "                if re.search(r'[a-z][A-Z]', word):\n",
    "                    # Split at capital letters and add parts individually\n",
    "                    parts = re.findall(r'[A-Z][a-z]*|[a-z]+', word)\n",
    "                    clean_words.extend([p.lower() for p in parts if p])\n",
    "                else:\n",
    "                    clean_words.append(word)\n",
    "        \n",
    "        print(f\"Total words after cleaning: {len(clean_words)}\")\n",
    "        return clean_words\n",
    "    \n",
    "    def _build_language_models(self, words: List[str]):\n",
    "        \"\"\"Build word pairs (bigrams) and triples (trigrams) for language modeling.\"\"\"\n",
    "        print(\"Building word pairs and triples...\")\n",
    "        \n",
    "        # Build word pairs (for bigram model)\n",
    "        for i in range(len(words)-1):\n",
    "            self.word_pairs[words[i]][words[i+1]] += 1\n",
    "        \n",
    "        # Build word triples (for trigram model)\n",
    "        for i in range(len(words)-2):\n",
    "            word1 = words[i]\n",
    "            word2 = words[i+1]\n",
    "            word3 = words[i+2]\n",
    "            \n",
    "            # Use the nested defaultdict structure\n",
    "            self.word_triples[word1][word2][word3] += 1\n",
    "        \n",
    "        print(\"Word pairs and triples built.\")\n",
    "\n",
    "    #################################\n",
    "    # Autocorrect Functions\n",
    "    #################################\n",
    "\n",
    "    def autocorrect(self, input_word: str, num_suggestions: int = 3) -> List[str]:\n",
    "        \"\"\"\n",
    "        Autocorrect the input word based on edit distance and bigram probabilities.\n",
    "        \n",
    "        Args:\n",
    "            input_word: The word to autocorrect\n",
    "            num_suggestions: The number of suggestions to return\n",
    "            \n",
    "        Returns:\n",
    "            A list of suggested corrections for the input word\n",
    "        \"\"\"\n",
    "        print(f\"Autocorrecting word: {input_word}\")\n",
    "        \n",
    "        # If the input word is in the vocabulary, return it immediately\n",
    "        if input_word in self.vocab:\n",
    "            print(\"Word found in vocabulary. No correction needed.\")\n",
    "            return [input_word]\n",
    "        \n",
    "        # Find candidate words with edit distance <= 2\n",
    "        candidates = self._get_candidates(input_word)\n",
    "        \n",
    "        # Score each candidate based on edit distance and bigram probabilities\n",
    "        scored_candidates = self._score_candidates(input_word, candidates)\n",
    "        \n",
    "        # Get the top N suggestions\n",
    "        top_suggestions = heapq.nlargest(num_suggestions, scored_candidates, key=lambda x: x[1])\n",
    "        \n",
    "        print(f\"Top {num_suggestions} suggestions: {top_suggestions}\")\n",
    "        \n",
    "        # Return only the suggested words\n",
    "        return [word for word, score in top_suggestions]\n",
    "\n",
    "    def _get_candidates(self, input_word: str, max_edit_distance: int = 2) -> Set[str]:\n",
    "        \"\"\"\n",
    "        Find candidate words within a maximum edit distance from the input word.\n",
    "        \n",
    "        Args:\n",
    "            input_word: The input word\n",
    "            max_edit_distance: The maximum edit distance to consider\n",
    "            \n",
    "        Returns:\n",
    "            A set of candidate words within the specified edit distance\n",
    "        \"\"\"\n",
    "        print(\"Finding candidate words...\")\n",
    "        \n",
    "        # Use words with edit distance of 1 and 2\n",
    "        words_ed_1 = self._edit_one_letter(input_word)\n",
    "        words_ed_2 = set()\n",
    "        for word in words_ed_1:\n",
    "            words_ed_2.update(self._edit_one_letter(word))\n",
    "            \n",
    "        # Filter candidates based on whether they are in the vocabulary\n",
    "        valid_candidates = {word for word in words_ed_1.union(words_ed_2) if word in self.vocab}\n",
    "        \n",
    "        print(f\"Found {len(valid_candidates)} candidate words.\")\n",
    "        return valid_candidates\n",
    "\n",
    "    def _edit_one_letter(self, word: str) -> Set[str]:\n",
    "        \"\"\"\n",
    "        Generate all possible words one edit distance away from the input word.\n",
    "        \n",
    "        Args:\n",
    "            word: The input word\n",
    "            \n",
    "        Returns:\n",
    "            A set of words one edit distance away from the input word\n",
    "        \"\"\"\n",
    "        letters = string.ascii_lowercase\n",
    "        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "        deletes = [L + R[1:] for L, R in splits if R]\n",
    "        inserts = [L + c + R for L, R in splits for c in letters]\n",
    "        replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
    "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "        \n",
    "        return set(deletes + inserts + replaces + transposes)\n",
    "\n",
    "    def _score_candidates(self, input_word: str, candidates: Set[str]) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Score candidate words based on edit distance and bigram probabilities.\n",
    "        \n",
    "        Args:\n",
    "            input_word: The input word\n",
    "            candidates: A set of candidate words\n",
    "            \n",
    "        Returns:\n",
    "            A list of tuples containing the candidate word and its score\n",
    "        \"\"\"\n",
    "        print(\"Scoring candidate words...\")\n",
    "        \n",
    "        # Edit distance scoring\n",
    "        edit_scores = {candidate: self._edit_distance(input_word, candidate) for candidate in candidates}\n",
    "\n",
    "        # Bigram probability scoring (Simple example - Modify for better scoring)\n",
    "        bigram_scores = {candidate: self._calculate_bigram_probability(input_word, candidate) for candidate in candidates}\n",
    "\n",
    "        # Combine edit distance and bigram probability scores\n",
    "        scored_candidates = [(candidate, (0.6 / (edit_scores[candidate] + 1)) + (0.4 * bigram_scores[candidate])) for candidate in candidates]\n",
    "\n",
    "        print(\"Candidate words scored.\")\n",
    "        return scored_candidates\n",
    "\n",
    "    def _edit_distance(self, word1: str, word2: str) -> int:\n",
    "        \"\"\"\n",
    "        Calculate the edit distance between two words.\n",
    "        \n",
    "        Args:\n",
    "            word1: The first word\n",
    "            word2: The second word\n",
    "            \n",
    "        Returns:\n",
    "            The edit distance between the two words\n",
    "        \"\"\"\n",
    "        len_word1 = len(word1)\n",
    "        len_word2 = len(word2)\n",
    "        \n",
    "        # Initialize a matrix to store edit distances\n",
    "        matrix = np.zeros((len_word1 + 1, len_word2 + 1), dtype=int)\n",
    "        \n",
    "        # Initialize the first row and first column\n",
    "        for i in range(len_word1 + 1):\n",
    "            matrix[i][0] = i\n",
    "        for j in range(len_word2 + 1):\n",
    "            matrix[0][j] = j\n",
    "        \n",
    "        # Calculate edit distances\n",
    "        for i in range(1, len_word1 + 1):\n",
    "            for j in range(1, len_word2 + 1):\n",
    "                if word1[i-1] == word2[j-1]:\n",
    "                    cost = 0\n",
    "                else:\n",
    "                    cost = 1\n",
    "                    \n",
    "                matrix[i][j] = min(matrix[i-1][j] + 1,      # Deletion\n",
    "                                    matrix[i][j-1] + 1,      # Insertion\n",
    "                                    matrix[i-1][j-1] + cost) # Substitution\n",
    "        \n",
    "        return matrix[len_word1][len_word2]\n",
    "\n",
    "    def _calculate_bigram_probability(self, prev_word: str, next_word: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the bigram probability of 'next_word' following 'prev_word'.\n",
    "\n",
    "        Args:\n",
    "            prev_word: The preceding word\n",
    "            next_word: The following word\n",
    "\n",
    "        Returns:\n",
    "            The bigram probability\n",
    "        \"\"\"\n",
    "        if prev_word in self.word_pairs and next_word in self.word_pairs[prev_word]:\n",
    "            count_prev = self.word_freq[prev_word]\n",
    "            count_bigram = self.word_pairs[prev_word][next_word]\n",
    "            return count_bigram / count_prev\n",
    "        else:\n",
    "            return 0.0  # If the bigram is not found, return 0\n",
    "\n",
    "    #################################\n",
    "    # Autocomplete Functions\n",
    "    #################################\n",
    "\n",
    "    def autocomplete(self, input_phrase: str, num_suggestions: int = 5) -> List[str]:\n",
    "        \"\"\"\n",
    "        Autocomplete the input phrase based on bigram probabilities.\n",
    "        \n",
    "        Args:\n",
    "            input_phrase: The input phrase\n",
    "            num_suggestions: The number of suggestions to return\n",
    "            \n",
    "        Returns:\n",
    "            A list of suggested words to complete the input phrase\n",
    "        \"\"\"\n",
    "        print(f\"Autocomplete suggestions for: {input_phrase}\")\n",
    "        \n",
    "        # Split the input phrase into words\n",
    "        words = input_phrase.lower().split()\n",
    "        \n",
    "        # Get the last word in the phrase\n",
    "        last_word = words[-1] if words else \"\"\n",
    "        \n",
    "        # Find candidate words that follow the last word\n",
    "        candidates = self._get_autocomplete_candidates(last_word)\n",
    "        \n",
    "        # Get the top N suggestions\n",
    "        top_suggestions = heapq.nlargest(num_suggestions, candidates, key=lambda x: x[1])\n",
    "        \n",
    "        print(f\"Top {num_suggestions} autocomplete suggestions: {top_suggestions}\")\n",
    "        \n",
    "        # Return only the suggested words\n",
    "        return [word for word, score in top_suggestions]\n",
    "\n",
    "    def _get_autocomplete_candidates(self, last_word: str) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Find candidate words that follow the last word in the input phrase.\n",
    "        \n",
    "        Args:\n",
    "            last_word: The last word in the input phrase\n",
    "            \n",
    "        Returns:\n",
    "            A list of tuples containing the candidate word and its probability\n",
    "        \"\"\"\n",
    "        print(\"Finding autocomplete candidates...\")\n",
    "        \n",
    "        # Retrieve the bigram probabilities for the last word\n",
    "        bigrams = self.word_pairs.get(last_word, {})\n",
    "        \n",
    "        # Convert bigrams to a list of tuples containing the candidate word and its probability\n",
    "        candidates = [(word, count) for word, count in bigrams.items()]\n",
    "        \n",
    "        print(f\"Found {len(candidates)} autocomplete candidates.\")\n",
    "        return candidates\n",
    "\n",
    "    #################################\n",
    "    # Next Word Prediction Function\n",
    "    #################################\n",
    "    \n",
    "    def predict_next_word(self, input_phrase: str, num_suggestions: int = 5) -> List[str]:\n",
    "        \"\"\"\n",
    "        Predict the next word in the input phrase based on trigram probabilities.\n",
    "        \n",
    "        Args:\n",
    "            input_phrase: The input phrase\n",
    "            num_suggestions: The number of suggestions to return\n",
    "            \n",
    "        Returns:\n",
    "            A list of suggested words to follow the input phrase\n",
    "        \"\"\"\n",
    "        print(f\"Predicting next word for: {input_phrase}\")\n",
    "        \n",
    "        # Split the input phrase into words\n",
    "        words = input_phrase.lower().split()\n",
    "        \n",
    "        # Ensure we have at least two words to form a trigram\n",
    "        if len(words) < 2:\n",
    "            print(\"Not enough words to predict next word. Provide at least two words.\")\n",
    "            return []\n",
    "        \n",
    "        # Get the last two words in the phrase\n",
    "        last_word1 = words[-2]\n",
    "        last_word2 = words[-1]\n",
    "        \n",
    "        # Find candidate words that follow the last two words\n",
    "        candidates = self._get_next_word_candidates(last_word1, last_word2)\n",
    "        \n",
    "        # Get the top N suggestions\n",
    "        top_suggestions = heapq.nlargest(num_suggestions, candidates, key=lambda x: x[1])\n",
    "        \n",
    "        print(f\"Top {num_suggestions} next word suggestions: {top_suggestions}\")\n",
    "        \n",
    "        # Return only the suggested words\n",
    "        return [word for word, score in top_suggestions]\n",
    "    \n",
    "    def _get_next_word_candidates(self, last_word1: str, last_word2: str) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Find candidate words that follow the last two words in the input phrase based on trigram probabilities.\n",
    "        \n",
    "        Args:\n",
    "            last_word1: The second-to-last word in the input phrase\n",
    "            last_word2: The last word in the input phrase\n",
    "            \n",
    "        Returns:\n",
    "            A list of tuples containing the candidate word and its probability\n",
    "        \"\"\"\n",
    "        print(\"Finding next word candidates...\")\n",
    "        \n",
    "        # Retrieve the trigram probabilities for the last two words\n",
    "        trigrams = self.word_triples.get(last_word1, {}).get(last_word2, {})\n",
    "        \n",
    "        # Convert trigrams to a list of tuples containing the candidate word and its probability\n",
    "        candidates = [(word, count) for word, count in trigrams.items()]\n",
    "        \n",
    "        print(f\"Found {len(candidates)} next word candidates.\")\n",
    "        return candidates\n",
    "\n",
    "# Example Usage (inside the class or separately)\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the SciFi Writing Assistant\n",
    "    assistant = SciFiWritingAssistant(corpus_file_path='/Users/stevgo/Downloads/corpus.txt')\n",
    "    \n",
    "    # Test the autocorrect function\n",
    "    word_to_correct = \"rocx\"\n",
    "    corrected_words = assistant.autocorrect(word_to_correct, num_suggestions=3)\n",
    "    print(f\"Autocorrect suggestions for '{word_to_correct}': {corrected_words}\")\n",
    "    \n",
    "    # Test the autocomplete function\n",
    "    phrase_to_complete = \"cute\"\n",
    "    completed_words = assistant.autocomplete(phrase_to_complete, num_suggestions=5)\n",
    "    print(f\"Autocomplete suggestions for '{phrase_to_complete}': {completed_words}\")\n",
    "    \n",
    "    # Test the next word prediction function\n",
    "    phrase_to_predict = \"lovely\"\n",
    "    predicted_words = assistant.predict_next_word(phrase_to_predict, num_suggestions=5)\n",
    "    print(f\"Next word suggestions for '{phrase_to_predict}': {predicted_words}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3340b42a",
   "metadata": {},
   "source": [
    "# 5. Evaluation of Model\n",
    "## 5a. Performance Metrics (10%)\n",
    "Since this is a language generation and correction task, we use qualitative evaluation:\n",
    "- Coherence of generated sentences\n",
    "- Accuracy of autocorrect predictions (manually tested)\n",
    "\n",
    "## 5b. Evaluation Code & Result\n",
    "We evaluate our model using the following code to generate:\n",
    "- A sentence from the bigram model\n",
    "- Autocorrect outputs for various intentionally misspelled words\n",
    "\n",
    "This demonstrates the qualitative performance of the language model and correction system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acad44ab",
   "metadata": {},
   "source": [
    "# 6. Conclusion & Future Work (5%)\n",
    "Our bigram and trigram models were able to generate reasonable Sci-Fi themed text based on the training corpus. The autocorrect system showed good potential in correcting common misspellings using both edit distance and word frequency.\n",
    "\n",
    "Future work:\n",
    "- Use of smoothing techniques for unseen n-grams\n",
    "- Implementation of transformer-based models (e.g., GPT)\n",
    "- Better evaluation using a held-out test set and BLEU/Perplexity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76faaf83-c3a0-45ba-b19c-cf7e1c8636f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
