{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05b5a1f2-151c-4640-bcd3-0c7891e0057a",
   "metadata": {},
   "source": [
    "# 1. Background Problem (20%)\n",
    "Language modeling is a fundamental task in Natural Language Processing (NLP), used in various applications like predictive typing, text generation, and spelling correction. For this project, I chose the Sci-Fi Stories Text Corpus available on Kaggle. Sci-Fi literature is linguistically rich and imaginative, often pushing boundaries of vocabulary and structure. Modeling such text is both challenging and rewarding, and it provides an exciting opportunity to explore how well statistical language models and autocorrect systems can handle complex and creative writing. Recent research has demonstrated that large language models can generalize to a wide variety of tasks, including creative text generation and spelling correction, even in few-shot settings. Studies have also shown that while language models are capable of producing creative writing, they face unique challenges in maintaining coherence and handling the imaginative language found in genres like science fiction. Furthermore, advances in spelling correction techniques have highlighted the importance of robust language modeling for correcting errors in creative and domain-specific texts\n",
    "\n",
    "References:\n",
    "* Brown, T. B., et al. (2020). Language Models are Few-Shot Learners.\n",
    "* Clark, E., et al. (2021). The Effectiveness of Language Models in Generating Creative Writing.\n",
    "* Zhang, Z., et al. (2022). A Survey on Spelling Correction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4472924f",
   "metadata": {},
   "source": [
    "# 2. Resource\n",
    "\n",
    "We used the following dataset found from kaggle:\n",
    "\n",
    "Sci-Fi Stories Text Corpus by Jannes Klaas: \n",
    "- https://www.kaggle.com/datasets/jannesklaas/scifi-stories-text-corpus\n",
    "\n",
    "The dataset contains a collection of sci-fi short stories in plain text, which provides an ideal source for both syntactic and lexical modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0eaaf9",
   "metadata": {},
   "source": [
    "# 3. Methods (10%)\n",
    "## We applied the following methods:\n",
    "\n",
    "- Preprocessing:\n",
    "    * Lowercasing all text\n",
    "    * Removing punctuation\n",
    "    * Tokenizing into words\n",
    "\n",
    "- Model Building:\n",
    "    * Bigram Language Model (word-based)\n",
    "    * Trigram Language Model\n",
    "\n",
    "- Advanced Method:\n",
    "    * Autocorrect using edit distance and bigram probability re-ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8e77a7",
   "metadata": {},
   "source": [
    "## 4. Model Implementation Code (50%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4623b240-3c74-474e-b2de-b0b703944ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading corpus from corpus.txt...\n",
      "Preprocessing corpus...\n",
      "Total words before cleaning: 31924829\n",
      "Total words after cleaning: 26330559\n",
      "Building word pairs and triples...\n",
      "Preprocessing complete.\n",
      "Corpus loaded and processed in 95.95 seconds\n",
      "Vocabulary size: 303305 words\n",
      "Bigram pairs: 5117856\n",
      "Trigram patterns: 14801024\n",
      "\n",
      "==================================================\n",
      "Sci-Fi Writing Assistant - Interactive Mode\n",
      "==================================================\n",
      "Commands:\n",
      "  - 'correct: [text]' - Autocorrect text\n",
      "  - 'complete: [prefix]' - Get completions for a prefix\n",
      "  - 'next: [text]' - Predict next word after text\n",
      "  - 'exit' - Quit the program\n",
      "  - Type any text to get suggestions as you write\n",
      "==================================================\n",
      "TIP: End your input with a space to get next word predictions.\n",
      "     Otherwise, you'll get word completions for the last word.\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Input:  hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggestions: hellos, hellop, hellofa, hellor, hello-goodby\n",
      "Next word predictions: he, to, hello, mr, there\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Input:  brother\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggestions: brothers, brotherhood, brother's, brother-in-law, brotherly\n",
      "Next word predictions: and, john, howard, had, was\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Input:  was\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggestions: wasn't, waste, washington, washed, wasted\n",
      "Next word predictions: a, the, not, no, in\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Input:  want\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggestions: wanted, wants, wanting, wanton, wanta\n",
      "Next word predictions: to, you, me, a, the\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import heapq\n",
    "from typing import List, Dict, Tuple, Set\n",
    "import os\n",
    "import time\n",
    "\n",
    "class SciFiWritingAssistant:\n",
    "    def __init__(self, corpus_file_path: str):\n",
    "        \"\"\"\n",
    "        Initialize the SciFi Writing Assistant with a corpus file.\n",
    "        \n",
    "        Args:\n",
    "            corpus_file_path: Path to the corpus text file\n",
    "        \"\"\"\n",
    "        self.corpus_file_path = corpus_file_path\n",
    "        self.word_freq = Counter()  # For autocorrect\n",
    "        self.vocab = set()  # All known words\n",
    "        self.word_pairs = defaultdict(Counter)  # For autocomplete\n",
    "        self.word_triples = defaultdict(lambda: defaultdict(Counter))  # For next word prediction\n",
    "        \n",
    "        self.load_and_preprocess_corpus()\n",
    "        \n",
    "    #################################\n",
    "    # Corpus Loading and Preprocessing\n",
    "    #################################\n",
    "    \n",
    "    def load_and_preprocess_corpus(self):\n",
    "        \"\"\"Load the corpus from file and preprocess it.\"\"\"\n",
    "        print(f\"Loading corpus from {self.corpus_file_path}...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            with open(self.corpus_file_path, 'r', encoding='utf-8') as file:\n",
    "                corpus_text = file.read()\n",
    "                \n",
    "            # Preprocess the loaded text\n",
    "            self._preprocess_text(corpus_text)\n",
    "            \n",
    "            self._print_corpus_stats(start_time)\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Could not find corpus file at {self.corpus_file_path}\")\n",
    "            self._load_minimal_corpus()\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading corpus: {str(e)}\")\n",
    "            self._load_minimal_corpus()\n",
    "    \n",
    "    def _load_minimal_corpus(self):\n",
    "        \"\"\"Load a minimal corpus as fallback.\"\"\"\n",
    "        print(\"Using a minimal default corpus instead.\")\n",
    "        minimal_corpus = \"\"\"\n",
    "        science fiction space robot alien technology future\n",
    "        i am going to the planet mars\n",
    "        i am not sure about this mission\n",
    "        i am ready for the journey\n",
    "        probably the best solution\n",
    "        probably we should try again\n",
    "        hello there my friend\n",
    "        hello to everyone here\n",
    "        brother and sister went home\n",
    "        the sister was happy\n",
    "        \"\"\"\n",
    "        self._preprocess_text(minimal_corpus)\n",
    "    \n",
    "    def _print_corpus_stats(self, start_time):\n",
    "        \"\"\"Print statistics about the loaded corpus.\"\"\"\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Corpus loaded and processed in {elapsed_time:.2f} seconds\")\n",
    "        print(f\"Vocabulary size: {len(self.vocab)} words\")\n",
    "        print(f\"Bigram pairs: {sum(len(v) for v in self.word_pairs.values())}\")\n",
    "        \n",
    "        # Calculate trigram count\n",
    "        trigram_count = 0\n",
    "        for key1 in self.word_triples:\n",
    "            for key2 in self.word_triples[key1]:\n",
    "                trigram_count += len(self.word_triples[key1][key2])\n",
    "        \n",
    "        print(f\"Trigram patterns: {trigram_count}\")\n",
    "    \n",
    "    def _preprocess_text(self, text: str):\n",
    "        \"\"\"Preprocess the corpus text to build vocabulary and word frequency.\"\"\"\n",
    "        print(\"Preprocessing corpus...\")\n",
    "        \n",
    "        # Clean and split the text\n",
    "        clean_words = self._clean_text(text)\n",
    "        \n",
    "        # Build vocabulary and word frequency\n",
    "        self.word_freq = Counter(clean_words)\n",
    "        self.vocab = set(clean_words)\n",
    "        \n",
    "        # Build word pairs and triples\n",
    "        self._build_language_models(clean_words)\n",
    "        \n",
    "        print(\"Preprocessing complete.\")\n",
    "    \n",
    "    def _clean_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Clean text and split into words.\"\"\"\n",
    "        # Convert to lowercase\n",
    "        cleaned_text = text.lower()\n",
    "        \n",
    "        # Add spaces around punctuation (except hyphens and apostrophes in words)\n",
    "        for p in set(string.punctuation) - {'-', \"'\"}:\n",
    "            cleaned_text = cleaned_text.replace(p, f' {p} ')\n",
    "        \n",
    "        # Fix joined words by adding spaces before capital letters in the middle of words\n",
    "        cleaned_text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', cleaned_text)\n",
    "        \n",
    "        # Split text by whitespace\n",
    "        words = cleaned_text.split()\n",
    "        \n",
    "        print(f\"Total words before cleaning: {len(words)}\")\n",
    "        \n",
    "        # Remove words with special characters and numbers only\n",
    "        clean_words = []\n",
    "        for word in words:\n",
    "            # Keep hyphenated words and contractions intact\n",
    "            word = word.strip(\"-'\")\n",
    "            # Skip empty words, numbers, and special character sequences\n",
    "            if word and not word.isdigit() and not re.match(r'^[#]+$', word) and not re.match(r'^[^\\w\\s]+$', word):\n",
    "                # Additional check for joined words without spaces\n",
    "                if re.search(r'[a-z][A-Z]', word):\n",
    "                    # Split at capital letters and add parts individually\n",
    "                    parts = re.findall(r'[A-Z][a-z]*|[a-z]+', word)\n",
    "                    clean_words.extend([p.lower() for p in parts if p])\n",
    "                else:\n",
    "                    clean_words.append(word)\n",
    "        \n",
    "        print(f\"Total words after cleaning: {len(clean_words)}\")\n",
    "        return clean_words\n",
    "    \n",
    "    def _build_language_models(self, words: List[str]):\n",
    "        \"\"\"Build word pairs (bigrams) and triples (trigrams) for language modeling.\"\"\"\n",
    "        print(\"Building word pairs and triples...\")\n",
    "        \n",
    "        # Build word pairs (for bigram model)\n",
    "        for i in range(len(words)-1):\n",
    "            self.word_pairs[words[i]][words[i+1]] += 1\n",
    "        \n",
    "        # Build word triples (for trigram model)\n",
    "        for i in range(len(words)-2):\n",
    "            word1 = words[i]\n",
    "            word2 = words[i+1]\n",
    "            word3 = words[i+2]\n",
    "            \n",
    "            # Use the nested defaultdict\n",
    "            self.word_triples[word1][word2][word3] += 1\n",
    "    \n",
    "    #################################\n",
    "    # Autocorrect Functionality\n",
    "    #################################\n",
    "    \n",
    "    def word_edits(self, word: str) -> Set[str]:\n",
    "        \"\"\"Generate all possible edits at edit distance 1 from the given word.\"\"\"\n",
    "        letters = string.ascii_lowercase + \"'-\"\n",
    "        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "        \n",
    "        # Deletion\n",
    "        deletes = [L + R[1:] for L, R in splits if R]\n",
    "        \n",
    "        # Transposition\n",
    "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
    "        \n",
    "        # Replacement\n",
    "        replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
    "        \n",
    "        # Insertion\n",
    "        inserts = [L + c + R for L, R in splits for c in letters]\n",
    "        \n",
    "        return set(deletes + transposes + replaces + inserts)\n",
    "    \n",
    "    def autocorrect(self, word: str, max_suggestions: int = 3) -> List[str]:\n",
    "        \"\"\"\n",
    "        Suggest corrections for a potentially misspelled word.\n",
    "        \n",
    "        Args:\n",
    "            word: The word to correct\n",
    "            max_suggestions: Maximum number of suggestions to return\n",
    "            \n",
    "        Returns:\n",
    "            List of suggested corrections\n",
    "        \"\"\"\n",
    "        if not word:\n",
    "            return []\n",
    "            \n",
    "        if word.lower() in self.vocab:\n",
    "            return [word]  # Word is correct\n",
    "        \n",
    "        # Generate candidates at edit distance 1\n",
    "        candidates = self.word_edits(word.lower())\n",
    "        valid_candidates = [w for w in candidates if w in self.vocab]\n",
    "        \n",
    "        # If no valid candidates, try edit distance 2\n",
    "        if not valid_candidates:\n",
    "            candidates_2 = set()\n",
    "            for candidate in candidates:\n",
    "                candidates_2.update(self.word_edits(candidate))\n",
    "            valid_candidates = [w for w in candidates_2 if w in self.vocab]\n",
    "        \n",
    "        # Sort by frequency in corpus\n",
    "        valid_candidates.sort(key=lambda x: self.word_freq[x], reverse=True)\n",
    "        \n",
    "        # Return top suggestions\n",
    "        return valid_candidates[:max_suggestions] if valid_candidates else [word]\n",
    "    \n",
    "    def correct_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Apply autocorrect to an entire text.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to correct\n",
    "            \n",
    "        Returns:\n",
    "            Corrected text\n",
    "        \"\"\"\n",
    "        # Split text into words and punctuation\n",
    "        tokens = re.findall(r'\\b[\\w\\'-]+\\b|[^\\w\\s]', text)\n",
    "        corrected_tokens = []\n",
    "        \n",
    "        for token in tokens:\n",
    "            if re.match(r'\\b[\\w\\'-]+\\b', token):\n",
    "                suggestions = self.autocorrect(token)\n",
    "                corrected_tokens.append(suggestions[0] if suggestions else token)\n",
    "            else:\n",
    "                corrected_tokens.append(token)  # Keep punctuation\n",
    "        \n",
    "        # Reconstruct text with proper spacing\n",
    "        result = \"\"\n",
    "        for i, token in enumerate(corrected_tokens):\n",
    "            if i > 0 and token not in string.punctuation:\n",
    "                result += \" \"\n",
    "            result += token\n",
    "                \n",
    "        return result\n",
    "    \n",
    "    #################################\n",
    "    # Autocomplete Functionality\n",
    "    #################################\n",
    "    \n",
    "    def autocomplete(self, prefix: str, context: str = None, max_suggestions: int = 5) -> List[str]:\n",
    "        \"\"\"\n",
    "        Suggest completions for a word prefix, optionally using context.\n",
    "        \n",
    "        Args:\n",
    "            prefix: The prefix to complete\n",
    "            context: Previous word(s) for context-aware completion\n",
    "            max_suggestions: Maximum number of suggestions to return\n",
    "            \n",
    "        Returns:\n",
    "            List of suggested completions\n",
    "        \"\"\"\n",
    "        if not prefix:\n",
    "            return []\n",
    "        \n",
    "        prefix = prefix.lower()\n",
    "        \n",
    "        # Find all words in vocabulary that start with prefix\n",
    "        candidates = [word for word in self.vocab if word.startswith(prefix)]\n",
    "        \n",
    "        # Ensure we're not suggesting words that are just the prefix itself\n",
    "        if prefix in candidates and len(candidates) > 1:\n",
    "            candidates.remove(prefix)\n",
    "        \n",
    "        # If context is provided, use it to refine suggestions\n",
    "        if context:\n",
    "            context_words = context.lower().split()\n",
    "            \n",
    "            if len(context_words) >= 2:\n",
    "                # Use trigram model\n",
    "                word1 = context_words[-2]\n",
    "                word2 = context_words[-1]\n",
    "                \n",
    "                if word1 in self.word_triples and word2 in self.word_triples[word1]:\n",
    "                    # Filter candidates by trigram context\n",
    "                    context_candidates = [\n",
    "                        word for word in candidates \n",
    "                        if word in self.word_triples[word1][word2]\n",
    "                    ]\n",
    "                    \n",
    "                    if context_candidates:\n",
    "                        context_candidates.sort(\n",
    "                            key=lambda x: self.word_triples[word1][word2][x], \n",
    "                            reverse=True\n",
    "                        )\n",
    "                        return context_candidates[:max_suggestions]\n",
    "            \n",
    "            # Fallback to bigram model\n",
    "            if context_words:\n",
    "                last_word = context_words[-1]\n",
    "                if last_word in self.word_pairs:\n",
    "                    # Filter candidates by those that appear after the context word\n",
    "                    context_candidates = [\n",
    "                        word for word in candidates \n",
    "                        if word in self.word_pairs[last_word]\n",
    "                    ]\n",
    "                    \n",
    "                    if context_candidates:\n",
    "                        context_candidates.sort(\n",
    "                            key=lambda x: self.word_pairs[last_word][x], \n",
    "                            reverse=True\n",
    "                        )\n",
    "                        return context_candidates[:max_suggestions]\n",
    "        \n",
    "        # Sort by overall frequency in corpus\n",
    "        candidates.sort(key=lambda x: self.word_freq[x], reverse=True)\n",
    "        return candidates[:max_suggestions]\n",
    "    \n",
    "    #################################\n",
    "    # Next Word Prediction\n",
    "    #################################\n",
    "    \n",
    "    def predict_next_word(self, text: str, max_suggestions: int = 5) -> List[str]:\n",
    "        \"\"\"\n",
    "        Predict the next word after a given text snippet.\n",
    "        \n",
    "        Args:\n",
    "            text: The text snippet to predict after\n",
    "            max_suggestions: Maximum number of suggestions to return\n",
    "            \n",
    "        Returns:\n",
    "            List of suggested next words\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            # Return common words if no context\n",
    "            return [word for word, _ in self.word_freq.most_common(max_suggestions)]\n",
    "        \n",
    "        # Clean and split the input text\n",
    "        text = text.lower()\n",
    "        # Remove punctuation for better matching\n",
    "        for p in string.punctuation:\n",
    "            text = text.replace(p, ' ')\n",
    "        words = text.split()\n",
    "        \n",
    "        # Use trigram model if we have at least 2 words\n",
    "        if len(words) >= 2:\n",
    "            word1 = words[-2]\n",
    "            word2 = words[-1]\n",
    "            \n",
    "            if word1 in self.word_triples and word2 in self.word_triples[word1]:\n",
    "                # Get all next words from trigram model\n",
    "                next_words = self.word_triples[word1][word2]\n",
    "                if next_words:\n",
    "                    # Sort by frequency\n",
    "                    sorted_words = sorted(next_words.items(), key=lambda x: x[1], reverse=True)\n",
    "                    return [word for word, _ in sorted_words[:max_suggestions]]\n",
    "        \n",
    "        # Fallback to bigram model\n",
    "        if words:\n",
    "            last_word = words[-1]\n",
    "            if last_word in self.word_pairs:\n",
    "                # Get all next words from bigram model\n",
    "                next_words = self.word_pairs[last_word]\n",
    "                if next_words:\n",
    "                    # Sort by frequency\n",
    "                    sorted_words = sorted(next_words.items(), key=lambda x: x[1], reverse=True)\n",
    "                    return [word for word, _ in sorted_words[:max_suggestions]]\n",
    "        \n",
    "        # Fallback to most common words\n",
    "        return [word for word, _ in self.word_freq.most_common(max_suggestions)]\n",
    "    \n",
    "    #################################\n",
    "    # Specialized Suggestions\n",
    "    #################################\n",
    "    \n",
    "    def get_specialized_suggestions(self, prefix: str, max_suggestions: int = 3) -> List[str]:\n",
    "        \"\"\"\n",
    "        Get specialized sci-fi related suggestions that start with the prefix.\n",
    "        \n",
    "        Args:\n",
    "            prefix: The prefix to match\n",
    "            max_suggestions: Maximum number of suggestions\n",
    "            \n",
    "        Returns:\n",
    "            List of sci-fi related suggestions\n",
    "        \"\"\"\n",
    "        # Define some common sci-fi terms\n",
    "        scifi_terms = [\n",
    "            \"spaceship\", \"starship\", \"asteroid\", \"galaxy\", \"universe\", \"teleport\",\n",
    "            \"robot\", \"android\", \"cyborg\", \"alien\", \"extraterrestrial\", \"humanoid\",\n",
    "            \"terraforming\", \"interstellar\", \"interplanetary\", \"cosmic\", \"quantum\",\n",
    "            \"wormhole\", \"nebula\", \"pulsar\", \"quasar\", \"telekinesis\", \"teleportation\",\n",
    "            \"lightyear\", \"supernova\", \"timeship\", \"hyperdrive\", \"stargate\", \"dystopian\",\n",
    "            \"utopian\", \"nanobot\", \"terraform\", \"holographic\", \"laser\", \"antimatter\",\n",
    "            \"warp\", \"subspace\", \"hyperspace\", \"cryogenic\", \"cryosleep\", \"singularity\",\n",
    "            \"nanite\", \"mecha\", \"artificial\", \"intelligence\", \"consciousness\", \"psychic\",\n",
    "            \"dimension\", \"parallel\", \"portal\", \"genetic\", \"enhancement\", \"neural\",\n",
    "            \"implant\", \"fusion\", \"radiation\", \"mutant\", \"mutation\", \"terraform\",\n",
    "            \"gravitational\", \"forcefield\", \"shield\", \"cybernetic\", \"augmentation\",\n",
    "            \"hologram\", \"simulation\", \"virtual\", \"reality\", \"colony\", \"colonization\"\n",
    "        ]\n",
    "        \n",
    "        # Extract sci-fi words from our corpus that are more frequent\n",
    "        corpus_scifi = [word for word in self.vocab \n",
    "                       if self.word_freq[word] >= 5 and len(word) > 4]\n",
    "        combined_vocab = set(scifi_terms + corpus_scifi)\n",
    "        \n",
    "        matches = [term for term in combined_vocab if term.startswith(prefix.lower())]\n",
    "        \n",
    "        # Sort by frequency in our corpus first, then by predefined list\n",
    "        matches.sort(key=lambda x: (-(x in self.vocab) * self.word_freq[x], x in scifi_terms, len(x)))\n",
    "        \n",
    "        return matches[:max_suggestions]\n",
    "    \n",
    "    #################################\n",
    "    # Interactive Mode\n",
    "    #################################\n",
    "    \n",
    "    def parse_user_input(self, user_input: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Parse user input to determine if it's a complete phrase or partial word.\n",
    "        \n",
    "        Args:\n",
    "            user_input: The user input string\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with parsed information\n",
    "        \"\"\"\n",
    "        result = {\n",
    "            'input': user_input,\n",
    "            'words': user_input.split(),\n",
    "            'last_word': '',\n",
    "            'last_word_complete': True,\n",
    "            'phrase': '',\n",
    "            'prefix': '',\n",
    "            'needs_next_word': False,\n",
    "            'needs_completion': False\n",
    "        }\n",
    "        \n",
    "        if not user_input.strip():\n",
    "            return result\n",
    "        \n",
    "        # Split into words\n",
    "        words = user_input.split()\n",
    "        result['words'] = words\n",
    "        \n",
    "        # Get the last word\n",
    "        last_word = words[-1] if words else ''\n",
    "        result['last_word'] = last_word\n",
    "        \n",
    "        # Check if the input ends with a space (complete phrase)\n",
    "        if user_input.endswith(' '):\n",
    "            result['phrase'] = user_input.strip()\n",
    "            result['needs_next_word'] = True\n",
    "        # Otherwise, treat as potentially partial word\n",
    "        else:\n",
    "            result['phrase'] = ' '.join(words[:-1]) if len(words) > 1 else ''\n",
    "            result['prefix'] = last_word\n",
    "            result['needs_completion'] = True\n",
    "            \n",
    "            # Check if the last word is likely complete\n",
    "            result['last_word_complete'] = last_word in self.vocab\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def interactive_mode(self):\n",
    "        \"\"\"Interactive mode for the writing assistant.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"Sci-Fi Writing Assistant - Interactive Mode\")\n",
    "        print(\"=\"*50)\n",
    "        print(\"Commands:\")\n",
    "        print(\"  - 'correct: [text]' - Autocorrect text\")\n",
    "        print(\"  - 'complete: [prefix]' - Get completions for a prefix\")\n",
    "        print(\"  - 'next: [text]' - Predict next word after text\")\n",
    "        print(\"  - 'exit' - Quit the program\")\n",
    "        print(\"  - Type any text to get suggestions as you write\")\n",
    "        print(\"=\"*50)\n",
    "        print(\"TIP: End your input with a space to get next word predictions.\")\n",
    "        print(\"     Otherwise, you'll get word completions for the last word.\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        context = []\n",
    "        while True:\n",
    "            user_input = input(\"\\nInput: \")\n",
    "            if user_input.lower() == 'exit':\n",
    "                print(\"Exiting interactive mode.\")\n",
    "                break\n",
    "                \n",
    "            # Add input to context (keep only last 10 words for context)\n",
    "            words = user_input.split()\n",
    "            context.extend(words)\n",
    "            if len(context) > 10:\n",
    "                context = context[-10:]\n",
    "            \n",
    "            # Check for correction\n",
    "            if user_input.lower().startswith('correct:'):\n",
    "                text_to_correct = user_input[8:].strip()\n",
    "                if text_to_correct:\n",
    "                    corrected = self.correct_text(text_to_correct)\n",
    "                    print(f\"Corrected: {corrected}\")\n",
    "                else:\n",
    "                    print(\"Please provide text to correct.\")\n",
    "            \n",
    "            # Check for completion\n",
    "            elif user_input.lower().startswith('complete:'):\n",
    "                prefix = user_input[9:].strip()\n",
    "                if prefix:\n",
    "                    context_str = ' '.join(context[:-1]) if len(context) > 1 else ''\n",
    "                    completions = self.autocomplete(prefix, context_str)\n",
    "                    spec_completions = self.get_specialized_suggestions(prefix)\n",
    "                    \n",
    "                    # Combine and deduplicate suggestions\n",
    "                    combined = []\n",
    "                    seen = set()\n",
    "                    \n",
    "                    # Prioritize specialized suggestions but keep diversity\n",
    "                    for suggestion in spec_completions + completions:\n",
    "                        if suggestion not in seen and len(combined) < 5:\n",
    "                            combined.append(suggestion)\n",
    "                            seen.add(suggestion)\n",
    "                    \n",
    "                    if combined:\n",
    "                        print(f\"Completions for '{prefix}': {', '.join(combined)}\")\n",
    "                    else:\n",
    "                        print(f\"No completions found for '{prefix}'\")\n",
    "                else:\n",
    "                    print(\"Please provide a prefix to complete.\")\n",
    "            \n",
    "            # Check for next word prediction\n",
    "            elif user_input.lower().startswith('next:'):\n",
    "                text = user_input[5:].strip()\n",
    "                if text:\n",
    "                    next_words = self.predict_next_word(text)\n",
    "                    if next_words:\n",
    "                        print(f\"Predicted next words after '{text}': {', '.join(next_words)}\")\n",
    "                    else:\n",
    "                        print(f\"No predictions found after '{text}'\")\n",
    "                else:\n",
    "                    print(\"Please provide text to predict after.\")\n",
    "            \n",
    "            else:\n",
    "                # Parse the user input to determine what kind of assistance to provide\n",
    "                parsed = self.parse_user_input(user_input)\n",
    "                \n",
    "                # Special handling for complete phrases (ending with space)\n",
    "                if parsed['needs_next_word']:\n",
    "                    next_words = self.predict_next_word(parsed['phrase'])\n",
    "                    if next_words:\n",
    "                        print(f\"Next word predictions: {', '.join(next_words)}\")\n",
    "                    else:\n",
    "                        print(\"No next word predictions available.\")\n",
    "                \n",
    "                # Special handling for partial words (not ending with space)\n",
    "                elif parsed['needs_completion'] and len(parsed['prefix']) >= 2:\n",
    "                    # Check if it needs correction\n",
    "                    if parsed['prefix'] not in self.vocab:\n",
    "                        corrections = self.autocorrect(parsed['prefix'])\n",
    "                        if corrections and corrections[0] != parsed['prefix']:\n",
    "                            print(f\"Did you mean: {', '.join(corrections)}?\")\n",
    "                    \n",
    "                    # Offer word completions \n",
    "                    completions = self.autocomplete(parsed['prefix'], parsed['phrase'])\n",
    "                    # Filter out completions that are the same as the prefix\n",
    "                    filtered_completions = [c for c in completions if c != parsed['prefix']]\n",
    "                    \n",
    "                    if filtered_completions:\n",
    "                        print(f\"Suggestions: {', '.join(filtered_completions)}\")\n",
    "                \n",
    "                # For complete inputs with multiple words\n",
    "                if len(parsed['words']) > 0:\n",
    "                    # Always offer next word predictions\n",
    "                    next_words = self.predict_next_word(user_input)\n",
    "                    if next_words and not user_input.endswith(' '):\n",
    "                        # Only show if not already shown above for space-ending inputs\n",
    "                        print(f\"Next word predictions: {', '.join(next_words)}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Default to corpus.txt in the current directory\n",
    "    corpus_path = 'corpus.txt'\n",
    "    \n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(corpus_path):\n",
    "        print(f\"Warning: {corpus_path} not found in the current directory.\")\n",
    "        # Prompt for an alternative path\n",
    "        alt_path = input(\"Enter the full path to corpus.txt or press Enter to use a minimal corpus: \").strip()\n",
    "        if alt_path:\n",
    "            corpus_path = alt_path\n",
    "    \n",
    "    # Initialize the writing assistant\n",
    "    assistant = SciFiWritingAssistant(corpus_path)\n",
    "    \n",
    "    # Run in interactive mode\n",
    "    assistant.interactive_mode()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3340b42a",
   "metadata": {},
   "source": [
    "# 5. Evaluation of Model\n",
    "## 5a. Performance Metrics (10%)\n",
    "Since this is a language generation and correction task, we use qualitative evaluation:\n",
    "- Coherence of generated sentences\n",
    "- Accuracy of autocorrect predictions (manually tested)\n",
    "\n",
    "## 5b. Evaluation Code & Result\n",
    "We evaluate our model using the following code to generate:\n",
    "- A sentence from the bigram model\n",
    "- Autocorrect outputs for various intentionally misspelled words\n",
    "\n",
    "This demonstrates the qualitative performance of the language model and correction system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Conclusion & Future Work (5%)\n",
    "Our bigram and trigram models were able to generate reasonable Sci-Fi themed text based on the training corpus. The autocorrect system showed good potential in correcting common misspellings using both edit distance and word frequency.\n",
    "\n",
    "Future work:\n",
    "- Use of smoothing techniques for unseen n-grams\n",
    "- Implementation of transformer-based models (e.g., GPT)\n",
    "- Better evaluation using a held-out test set and BLEU/Perplexity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76faaf83-c3a0-45ba-b19c-cf7e1c8636f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
