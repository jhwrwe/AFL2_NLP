{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05b5a1f2-151c-4640-bcd3-0c7891e0057a",
   "metadata": {},
   "source": [
    "## 1. Background problem\n",
    "\n",
    "Language-based applications — such as search assistants, writing aids, and chatbots — often struggle with unconventional and imaginative text, especially in genres like science fiction. We chose the Sci-Fi Stories Corpus because it presents a unique challenge: the vocabulary is diverse, and often includes neologisms, non-standard grammar, and context-rich sentence structures.\n",
    "\n",
    "By building an Autocorrect + Autocomplete model, we aim to support use cases where:\n",
    "\n",
    "1. Readers want to search or explore sci-fi literature more easily\n",
    "2. Writers want support while composing sci-fi content\n",
    "3. NLP models require preprocessing support to deal with such unorthodox language usage\n",
    "4. This dual-model setup addresses both input correction and forward-suggestion, making it a practical tool 5. for improving accessibility and interaction with such text-heavy, creatively driven datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4472924f",
   "metadata": {},
   "source": [
    "## 2. Resource\n",
    "\n",
    "We used the following dataset found from kaggle:\n",
    "\n",
    "Sci-Fi Stories Text Corpus by Jannes Klaas: \n",
    "1. https://www.kaggle.com/datasets/jannesklaas/scifi-stories-text-corpus\n",
    "\n",
    "The dataset contains a collection of sci-fi short stories in plain text, which provides an ideal source for both syntactic and lexical modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0eaaf9",
   "metadata": {},
   "source": [
    "## 3. Methods (10%)\n",
    "### Pre-processing\n",
    "1. Tokenization using nltk.word_tokenize\n",
    "2. Lowercasing\n",
    "3. Removing punctuation and non-alphabetic characters\n",
    "4. Building a vocabulary index\n",
    "\n",
    "###  Steps of Model Building\n",
    "#### Autocorrect:\n",
    "2. Levenshtein Distance algorithm for correction\n",
    "3. N-gram frequency-based word suggestion\n",
    "4. Threshold filtering for out-of-vocabulary terms\n",
    "#### Autocomplete:\n",
    "2. Bigram language modeling with smoothed probabilities\n",
    "3. Suggestion ranking based on previous word context\n",
    "4. Backoff strategy for unseen word combinations\n",
    "\n",
    "### Advanced Method (Optional):\n",
    "1. Used word2vec for semantic similarity in autocomplete suggestions\n",
    "2. POS-based filtering for ensuring syntactically relevant completions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8e77a7",
   "metadata": {},
   "source": [
    "## 4. Model Implementation Code (50%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3340b42a",
   "metadata": {},
   "source": [
    "## 5. Evaluation of Model\n",
    "### 5a. Performance Metrics (10%)\n",
    "We use the following metrics:\n",
    "1. Accuracy of corrections for Autocorrect (compared to manually introduced typos)\n",
    "2. Top-k Precision for Autocomplete suggestions\n",
    "3. Keystroke savings (KS) for evaluating autocomplete efficiency\n",
    "\n",
    "### 5b. Evaluation Code & Result\n",
    "1. For Autocorrect: ~86% accuracy on test samples with intentional typos\n",
    "2. For Autocomplete: Top-3 Precision reached 78%\n",
    "3. Average KS: 24%, showing significant efficiency gain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2894f327",
   "metadata": {},
   "source": [
    "## 6. Conclusion & Future Work (5%)\n",
    "Our model performs reasonably well, especially considering the creative and irregular language in the corpus. The Autocorrect + Autocomplete combination works hand-in-hand to ensure both accuracy and suggestion quality.\n",
    "\n",
    "### Future Work:\n",
    "\n",
    "Integrating contextual embeddings (e.g., BERT) for deeper semantic understanding\n",
    "Expanding vocabulary coverage with character-level modeling\n",
    "Building a real-time interface (e.g., web-based demo) to test usability in creative writing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
