{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05b5a1f2-151c-4640-bcd3-0c7891e0057a",
   "metadata": {},
   "source": [
    "# 1. Background Problem (20%)\n",
    "Harnessing AI for creative writing can transform solitary struggle into collaborative inspiration, but generic language models routinely miss the mark on genre flavor, narrative pace, and invented terminology that define science fiction. Writers report that off-the-shelf tools often produce bland, out-of-place continuations,undermining creative flow rather than sustaining it \n",
    "\n",
    "Domain-specific corpora have been shown to dramatically boost both relevance and fluency in specialized tasks: e-commerce chatbots, legal text analysis, and medical assistants all achieve superior accuracy once fine-tuned on in-domain data. By analogy, a Sci-Fi Writing Assistant trained on general newswire or web text will lack the concepts, neologisms, and stylistic tropes (“hyperspanner,” “chronoflux,” the cadence of first-person interstellar monologues) that make science fiction compelling \n",
    "\n",
    "\n",
    "The SciFi Stories Text Corpus on Kaggle (142 MB of pulp-era tales and modern fan-fiction) provides ~25 M tokens of pure science fiction, drawn from the Pulp Magazine Archive and curated by Jannes Klaas . This volume is sufficient to learn robust bigram/trigram patterns and POS distributions without drowning in generic English structure .\n",
    "\n",
    "\n",
    "Grounding our Autocorrect, POS-Tagging, and Autocomplete model in this corpus addresses two core challenges:\n",
    "\n",
    "* Vocabulary Coverage and OOV Handling: reliably modeling speculative neologisms (e.g., “hyperspanner,” “chronoflux”) through observed usage patterns rather than extrapolating from general English.\n",
    "\n",
    "\n",
    "* Genre-Specific Syntax and Stylistics: capturing the narrative pacing, dialogue conventions, and descriptive flourishes unique to science fiction storytelling, which differ markedly from journalistic or conversational registers.\\\n",
    "\n",
    "In sum, a sci-fi–centric corpus is not a luxury but a necessity for an AI collaborator that feels like a seasoned genre writer—giving authors the right words at the right moment to ignite their worlds.\n",
    "\n",
    "### References:\n",
    "Ippolito, D., Yuan, A., Coenen, A., & Burnam, S. (2022). Creative Writing with an AI-Powered Writing Assistant: Perspectives from Professional Writers. arXiv. \n",
    "arXiv\n",
    "\n",
    "Guo, A., Sathyanarayanan, S., Wang, L., Heer, J., & Zhang, A. (2024). From Pen to Prompt: How Creative Writers Integrate AI into their Writing Practice. arXiv. \n",
    "arXiv\n",
    "\n",
    "Kili Technology. (2023). Building Domain-Specific LLMs: Examples and Techniques. Kili Blog. \n",
    "kili-website\n",
    "\n",
    "Analytics Vidhya. (2023). Unleashing the Potential of Domain-Specific LLMs. Analytics Vidhya. \n",
    "Analytics Vidhya\n",
    "\n",
    "Asgari, E., & Rezapour, M. (2023). Generative AI and the end of corpus-assisted data-driven learning. Computers and Education Open. \n",
    "ScienceDirect\n",
    "\n",
    "Noel B. (2024). Enhanced Fine-Tuning Techniques for Domain-Specific AI. Medium. \n",
    "Medium\n",
    "\n",
    "Jannes Klaas. (2017). SciFi Stories Text Corpus [Data set]. Kaggle.\n",
    "\n",
    "Buz, T., Frost, B., Genchev, N., Schneider, M., Kaffee, L.-A., & de Melo, G. (2024). Investigating Wit, Creativity, and Detectability of LLMs in Domain-Specific Writing Style Adaptation. arXiv. \n",
    "arXiv\n",
    "\n",
    "Piantadosi, S. T., Moran, R., & Roberts, J. (2020). A standardized Project Gutenberg corpus for statistical analysis of natural language and quantitative linguistics. PLOS Computational Biology, 16(3), e1007576. https://doi.org/10.1371/journal.pcbi.1007576\n",
    "\n",
    "Jurafsky, D., & Martin, J. H. (2021). Speech and Language Processing (3rd ed.). Prentice Hall.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4472924f",
   "metadata": {},
   "source": [
    "# 2. Resource\n",
    "\n",
    "We used the following dataset found from kaggle:\n",
    "\n",
    "Sci-Fi Stories Text Corpus by Jannes Klaas: \n",
    "- https://www.kaggle.com/datasets/jannesklaas/scifi-stories-text-corpus\n",
    "\n",
    "The dataset contains a collection of sci-fi short stories in plain text, which provides an ideal source for both syntactic and lexical modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0eaaf9",
   "metadata": {},
   "source": [
    "## 3. Methods (10%)\n",
    "\n",
    "### a) Preprocessing\n",
    "\n",
    "1. **Text Cleaning & Tokenization**  \n",
    "   - Read the raw sci-fi text line by line, convert to lowercase, and extract only alphanumeric tokens.  \n",
    "   - Split on sentence delimiters to form sentence units, then re-tokenize each sentence into a list of word tokens.  \n",
    "   - Wrap each sentence with `<s>` and `<e>` markers so the language model recognizes explicit boundaries.\n",
    "\n",
    "2. **Vocabulary Construction & OOV Handling**  \n",
    "   - Aggregate token frequencies across all sentences.  \n",
    "   - Discard any token seen only once, replacing those with a single `<unk>` placeholder.  \n",
    "   - This yields a stable, manageable vocabulary and prevents noise from one-off typos or names.\n",
    "\n",
    "3. **POS-Tagging Data Prep**  \n",
    "   - From the WSJ `.pos` corpus, extract parallel sequences of words and their POS tags.  \n",
    "   - Map any out-of-vocabulary items into broad unknown categories (digits, punctuation, uppercase forms, or morphology-based buckets) so the HMM can handle novel tokens gracefully.\n",
    "\n",
    "\n",
    "### b) Steps of building the models\n",
    "\n",
    "1. **Autocorrect Component**  \n",
    "   - Build unigram frequency counts of every word in the corpus.  \n",
    "   - For each misspelling, generate edit-distance variants and pick the correction with the highest unigram probability.\n",
    "\n",
    "2. **Smoothed N-gram Language Model**  \n",
    "   - Extract bigrams and trigrams (including `<s>`/`<e>` markers) from the tokenized sentences.  \n",
    "   - Count their occurrences, then convert counts into probabilities via Laplace (add-one) smoothing.  \n",
    "   - This ensures every observed or unobserved sequence has a small nonzero likelihood.\n",
    "\n",
    "3. **HMM-Based POS Tagger**  \n",
    "   - Compute tag transition probabilities (how often each POS follows another) and emission probabilities (how often each word is generated by each tag) from the tagged data.  \n",
    "   - Apply Laplace smoothing so that even rare transitions or emissions receive minimal probability mass.\n",
    "\n",
    "4. **Combined Autocomplete Mechanism**  \n",
    "   - Given a user’s partial input, first evaluate each candidate continuation’s fit under the n-gram model (contextual probability) and under the HMM (grammatical probability).  \n",
    "   - Fuse these two scores—using a tunable weight—to rank and present the top suggestions.\n",
    "\n",
    "\n",
    "### c) (Optional) Advanced Extensions\n",
    "\n",
    "- **Dynamic Backoff Strategy**  \n",
    "  Automatically back off from trigrams to bigrams to unigrams when encountering unseen histories.\n",
    "\n",
    "- **Adaptive Smoothing**  \n",
    "  Adjust the smoothing constant based on context sparsity: smaller for frequent histories, larger for rare ones.\n",
    "\n",
    "- **Efficient Data Structures**  \n",
    "  Store all count tables in hash maps and represent probability matrices compactly to keep both lookups fast and memory usage low.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8e77a7",
   "metadata": {},
   "source": [
    "## 4. Model Implementation Code (50%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93420f5-e5c9-412f-9005-83e89379d370",
   "metadata": {},
   "source": [
    "## 1. importing libraries and files needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ceab946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8afed5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsj_train_file = \"WSJ_02-21.pos\"\n",
    "hmm_vocab_file = \"hmm_vocab.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290d55fb-8105-46b4-a684-e366d81c487c",
   "metadata": {},
   "source": [
    "## 2.Data preprocessing\n",
    "\n",
    "This is to read, clean and tokenize the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0e2d6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(file_name):\n",
    "    #Reads a file, processes each line to lowercase, and extracts all words into a list.\n",
    "    words = []\n",
    "    with open(file_name, 'r', encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            line = line.lower()\n",
    "            w = re.findall(r'\\w+', line)\n",
    "            words += w\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e1e8b4-3e65-4d73-8276-bde984eb1ae2",
   "metadata": {},
   "source": [
    "## 3.making the N-gram model\n",
    "\n",
    "this is to create and count the n-grams, and estimate the  probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b9168f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counts(word_list):\n",
    "    #Returns a dictionary mapping each word to its frequency in the word list.\n",
    "    word_counts = {}\n",
    "    for word in word_list:\n",
    "        word_counts[word] = word_counts.get(word, 0) + 1\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bae2a85-20c7-4656-b9e7-ac2dc58d99e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probs(word_counts):\n",
    "    #Returns a dictionary mapping words to their probabilities based on given word counts.\n",
    "    total_words = sum(word_counts.values())\n",
    "    word_probs = {word: count / total_words for word, count in word_counts.items()}\n",
    "    return word_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27ae16b8-8b6f-4805-ab1c-7821dca98c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_sentences(text):\n",
    "    #Splits input text into a list of sentences using punctuation marks [.?!] as delimiters.\n",
    "    sentences = re.split(r'[.?!]+', text)\n",
    "    sentences = [s.strip() for s in sentences if s]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0de3992-d0ef-4496-81f8-3bdf37572ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(sentences):\n",
    "    #Converts sentences to lowercase and splits them into word tokens using regex.\n",
    "    tokenized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower()\n",
    "        tokens = re.findall(r'\\w+', sentence)\n",
    "        tokenized_sentences.append(tokens)\n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f68984fd-bd30-4725-8bd1-40a15e2f623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(tokenized_sentences, threshold=2):\n",
    "    #Generates vocabulary from tokenized sentences filtering words below frequency threshold.\n",
    "    word_counts = {}\n",
    "    for sentence in tokenized_sentences:\n",
    "        for token in sentence:\n",
    "            word_counts[token] = word_counts.get(token, 0) + 1\n",
    "\n",
    "    vocabulary = [word for word, count in word_counts.items() if count >= threshold]\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe7908ed-cb8a-407f-afff-21af911443b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_oov(tokenized_sentences, vocabulary, unknown_token=\"<unk>\"):\n",
    "    #Replaces out-of-vocabulary words in tokenized sentences with specified unknown token.\n",
    "    replaced_sentences = []\n",
    "    vocabulary = set(vocabulary)\n",
    "    for sentence in tokenized_sentences:\n",
    "        replaced_sentence = [token if token in vocabulary else unknown_token for token in sentence]\n",
    "        replaced_sentences.append(replaced_sentence)\n",
    "    return replaced_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5dee74d-2345-48e8-9eb6-05d064783ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_n_grams(tokenized_sentences, n):\n",
    "    #Generates n-grams from tokenized sentences with <s> and <e> markers.\n",
    "    n_grams = []\n",
    "    for sentence in tokenized_sentences:\n",
    "        sentence = [\"<s>\"] + sentence + [\"<e>\"]\n",
    "        for i in range(len(sentence) - n + 1):\n",
    "            n_grams.append(tuple(sentence[i:i+n]))\n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d659c205-31b1-4e78-af58-870b3d6c99b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_gram_counts(n_grams):\n",
    "    #Counts occurrences of each n-gram in the provided list.\n",
    "    n_gram_counts = {}\n",
    "    for n_gram in n_grams:\n",
    "        n_gram_counts[n_gram] = n_gram_counts.get(n_gram, 0) + 1\n",
    "    return n_gram_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab3a5ef8-7341-461b-87c8-4a6dfee7d0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_probability(word, previous_n_gram, n_gram_counts, n_minus_1_gram_counts, vocabulary_size, k=1.0):\n",
    "    #Calculates smoothed probability of word given previous n-gram using k-smoothing.\n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "    n_gram = previous_n_gram + (word,)\n",
    "    n_gram_count = n_gram_counts.get(n_gram, 0)\n",
    "    n_minus_1_gram_count = n_minus_1_gram_counts.get(previous_n_gram, 0)\n",
    "    probability = (n_gram_count + k) / (n_minus_1_gram_count + k * vocabulary_size)\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a4b6a91-2483-4799-a7d4-11c9260558c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_probabilities(previous_n_gram, n_gram_counts, n_minus_1_gram_counts, vocabulary, k=1.0):\n",
    "    #Estimates probabilities for all vocabulary words given previous n-gram.\n",
    "    probabilities = {}\n",
    "    for word in vocabulary:\n",
    "        probabilities[word] = estimate_probability(word, previous_n_gram,\n",
    "                                                   n_gram_counts, n_minus_1_gram_counts,\n",
    "                                                   len(vocabulary), k=k)\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51c8d987-0098-47e9-a4ab-1835f9e42584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_suggestions(previous_tokens, n_gram_counts, n_minus_1_gram_counts, vocabulary, k=1.0, start_with=None):\n",
    "    #Generates word suggestions based on previous tokens, optionally filtered by starting characters.\n",
    "    n = len(list(n_gram_counts.keys())[0])\n",
    "    previous_n_gram = previous_tokens[-n+1:]\n",
    "    probabilities = estimate_probabilities(previous_n_gram,\n",
    "                                           n_gram_counts, n_minus_1_gram_counts,\n",
    "                                           vocabulary, k=k)\n",
    "    suggestions = sorted(probabilities.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Filter out unknown word tokens from suggestions\n",
    "    suggestions = [s for s in suggestions if not s[0].startswith('--unk')]\n",
    "\n",
    "    if start_with:\n",
    "        suggestions = [s for s in suggestions if s[0].startswith(start_with)]\n",
    "\n",
    "    return suggestions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5bc918-0737-49cb-8b96-ff03c9f40303",
   "metadata": {},
   "source": [
    "## 5.adding pos tagging fucntion\n",
    "\n",
    "we will be adding pos tagging function in here and also for handling unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "388b6452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def assign_unk(tok):\n",
    "    # Assign unknown word tokens\n",
    "    punct = set(string.punctuation)\n",
    "    noun_suffix = [\"action\", \"age\", \"ance\", \"cy\", \"dom\", \"ee\", \"ence\", \"er\", \"hood\", \"ion\", \"ism\", \"ist\", \"ity\", \"ling\", \"ment\", \"ness\", \"or\", \"ry\", \"scape\", \"ship\", \"ty\"]\n",
    "    verb_suffix = [\"ate\", \"ify\", \"ise\", \"ize\"]\n",
    "    adj_suffix = [\"able\", \"ese\", \"ful\", \"i\", \"ian\", \"ible\", \"ic\", \"ish\", \"ive\", \"less\", \"ly\", \"ous\"]\n",
    "    adv_suffix = [\"ward\", \"wards\", \"wise\"]\n",
    "\n",
    "    if any(char.isdigit() for char in tok):\n",
    "        return \"--unk_digit--\"\n",
    "    elif any(char in punct for char in tok):\n",
    "        return \"--unk_punct--\"\n",
    "    elif any(char.isupper() for char in tok):\n",
    "        return \"--unk_upper--\"\n",
    "    elif any(tok.endswith(suffix) for suffix in noun_suffix):\n",
    "        return \"--unk_noun--\"\n",
    "    elif any(tok.endswith(suffix) for suffix in verb_suffix):\n",
    "        return \"--unk_verb--\"\n",
    "    elif any(tok.endswith(suffix) for suffix in adj_suffix):\n",
    "        return \"--unk_adj--\"\n",
    "    elif any(tok.endswith(suffix) for suffix in adv_suffix):\n",
    "        return \"--unk_adv--\"\n",
    "    return \"--unk--\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d223024-b959-4302-968d-97df6dc317f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_tag(line, vocab):\n",
    "    # Get the word and tag from a line of the training corpus\n",
    "    if not line.split():\n",
    "        word = \"--n--\"\n",
    "        tag = \"--s--\"\n",
    "        return word, tag\n",
    "    else:\n",
    "        word, tag = line.split('\\t')\n",
    "        word = word.strip()\n",
    "        tag = tag.strip()\n",
    "        if word not in vocab:\n",
    "            word = assign_unk(word)\n",
    "        return word, tag\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "221be45c-90a4-4fb6-aed1-8e44be7302f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(vocab, data_fp):\n",
    "    \"\"\"\n",
    "    Preprocess data\n",
    "    \"\"\"\n",
    "    orig = []\n",
    "    prep = []\n",
    "\n",
    "    # Read data\n",
    "    with open(data_fp, \"r\") as data_file:\n",
    "\n",
    "        for cnt, line in enumerate(data_file):\n",
    "\n",
    "            # Get the word tag pair\n",
    "            try:\n",
    "              word, tag = get_word_tag(line, vocab)\n",
    "            except:\n",
    "              continue #Skip anything that does not have a line\n",
    "\n",
    "            #Append the original word\n",
    "            orig.append(word)\n",
    "\n",
    "            #Check if the word is in vocab:\n",
    "            if word not in vocab:\n",
    "              word = assign_unk(word)\n",
    "\n",
    "            # Append preprocessed words\n",
    "            prep.append(word)\n",
    "\n",
    "\n",
    "    return orig, prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05774774-a883-424c-b564-cd736f98859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionaries(training_corpus):\n",
    "    \"\"\"\n",
    "    Create word and tag dictionaries.\n",
    "    \"\"\"\n",
    "    word_counts = defaultdict(int)\n",
    "    tag_counts = defaultdict(int)\n",
    "    transition_counts = defaultdict(int)\n",
    "    emission_counts = defaultdict(int)\n",
    "    \n",
    "    prev_tag = \"--s--\"\n",
    "    \n",
    "    i = 0\n",
    "    for word_tag in training_corpus:\n",
    "        i += 1\n",
    "        if i % 50000 == 0:\n",
    "            print(f\"read {i} words\")\n",
    "            \n",
    "        word, tag = get_word_tag(word_tag, vocab)\n",
    "        word_counts[word] += 1\n",
    "        tag_counts[tag] += 1\n",
    "        transition_counts[(prev_tag, tag)] += 1\n",
    "        emission_counts[(tag, word)] += 1\n",
    "        prev_tag = tag\n",
    "    \n",
    "    return word_counts, tag_counts, transition_counts, emission_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1472dfa-673a-416b-b1d3-a6b1a6ad67ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pos_model(training_corpus, vocab):\n",
    "    #Creates HMM model (transition matrix A and emission matrix B) from training corpus and vocabulary.\n",
    "\n",
    "    word_counts, tag_counts, transition_counts, emission_counts = create_dictionaries(training_corpus)\n",
    "    \n",
    "    tags = sorted(tag_counts.keys())\n",
    "    num_tags = len(tags)\n",
    "    A = np.zeros((num_tags, num_tags))\n",
    "    \n",
    "    for i in range(num_tags):\n",
    "        for j in range(num_tags):\n",
    "            A[i, j] = (transition_counts[(tags[i], tags[j])] + 1) / (tag_counts[tags[i]] + num_tags)\n",
    "    \n",
    "    B = defaultdict(lambda: defaultdict(float))\n",
    "    all_words = set(word_counts.keys())\n",
    "    \n",
    "    for tag in tags:\n",
    "        for word in all_words:\n",
    "            B[tag][word] = (emission_counts[(tag, word)] + 1) / (tag_counts[tag] + len(all_words))\n",
    "    \n",
    "    return A, B, tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac8c556-824b-4e92-91d1-eb903dfa8fdf",
   "metadata": {},
   "source": [
    "## 6.Autocomplete model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc31b8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def autocomplete(input_str, n_gram_counts, n_minus_1_gram_counts, vocabulary, A, B, tags, k=1.0, num_suggestions=5):\n",
    "    #Generates autocomplete suggestions for input string using combined N-gram and POS tagger probabilities.\n",
    "    tokens = re.findall(r'\\w+', input_str.lower())  # Tokenize the input\n",
    "    \n",
    "    # If there are no tokens, return an empty list\n",
    "    if not tokens:\n",
    "        return []\n",
    "    \n",
    "    # Get the POS predictions for the tokens from the training data\n",
    "    predicted_words = predict_next_word(input_str, n_gram_counts, n_minus_1_gram_counts, vocabulary, A, B, tags)  # Predict next words\n",
    "    \n",
    "    return predicted_words  # Return the predicted words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a43452-635f-49e6-bc81-424d73569ccc",
   "metadata": {},
   "source": [
    "## 7 combines POS tagging and N-gram probabilities\n",
    "\n",
    "this is used to predict the next word that will come up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c8403c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(states, corpus, vocab):\n",
    "    #Initializes HMM parameters.\n",
    "    \n",
    "    A = np.zeros((len(states), len(states)))\n",
    "    B = defaultdict(lambda: defaultdict(float))\n",
    "    \n",
    "    tag_counts = defaultdict(int)\n",
    "    word_counts = defaultdict(int)\n",
    "    \n",
    "    prev_tag = \"--s--\"\n",
    "    \n",
    "    for word_tag in corpus:\n",
    "        word, tag = get_word_tag(word_tag, vocab)\n",
    "        \n",
    "        tag_counts[tag] += 1\n",
    "        word_counts[word] += 1\n",
    "        \n",
    "        transition_counts = defaultdict(int)\n",
    "        emission_counts = defaultdict(int)\n",
    "\n",
    "        emission_counts[(tag, word)] += 1\n",
    "        transition_counts[(prev_tag, tag)] += 1\n",
    "        \n",
    "        prev_tag = tag\n",
    "    \n",
    "    return A, B, tag_counts, word_counts, transition_counts, emission_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "92097387-9b56-4e2e-b939-eefd5bb43ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transition_matrix(A, transition_counts, tag_counts, states):\n",
    "    #Creates a transition matrix from transition counts and tag counts.\n",
    "    num_states = len(states)\n",
    "    \n",
    "    for i in range(num_states):\n",
    "        for j in range(num_states):\n",
    "            A[i, j] = (transition_counts[(states[i], states[j])] + 1) / (tag_counts[states[i]] + num_states)\n",
    "            \n",
    "    return A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1f6ffe30-83bd-4020-8ef3-2fb2618101d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emission_matrix(B, emission_counts, tag_counts, vocab):\n",
    "    #Creates an emission matrix from emission counts, tag counts, and the vocabulary.\n",
    "    all_words = set(vocab.keys())\n",
    "    \n",
    "    for tag in tag_counts:\n",
    "        for word in all_words:\n",
    "            B[tag][word] = (emission_counts[(tag, word)] + 1) / (tag_counts[tag] + len(vocab))\n",
    "            \n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ec954a39-d393-4882-aca4-7005a3764ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(words, vocab, A, B, tags):\n",
    "    #Implements the Viterbi algorithm for POS tagging.\n",
    "\n",
    "    num_tags = len(tags)\n",
    "    num_words = len(words)\n",
    "    \n",
    "    best_probs = np.zeros((num_tags, num_words))\n",
    "    best_paths = np.zeros((num_tags, num_words), dtype=int)\n",
    "    \n",
    "    first_word = words[0]\n",
    "    for i in range(num_tags):\n",
    "        if first_word in vocab:\n",
    "            best_probs[i, 0] = B[tags[i]][first_word]\n",
    "        else:\n",
    "            best_probs[i, 0] = B[tags[i]][assign_unk(first_word)]\n",
    "    \n",
    "    for j in range(1, num_words):\n",
    "        for i in range(num_tags):\n",
    "            best_prob = float('-inf')\n",
    "            best_path = None\n",
    "            \n",
    "            for k in range(num_tags):\n",
    "                prob = best_probs[k, j-1] * A[k, i]\n",
    "                if words[j] in vocab:\n",
    "                    prob *= B[tags[i]][words[j]]\n",
    "                else:\n",
    "                    prob *= B[tags[i]][assign_unk(words[j])]\n",
    "                    \n",
    "                if prob > best_prob:\n",
    "                    best_prob = prob\n",
    "                    best_path = k\n",
    "            \n",
    "            best_probs[i, j] = best_prob\n",
    "            best_paths[i, j] = best_path\n",
    "    \n",
    "    tag_sequence = [None] * num_words\n",
    "    \n",
    "    z = np.argmax(best_probs[:, -1])\n",
    "    tag_sequence[-1] = tags[z]\n",
    "    \n",
    "    for i in range(num_words-2, -1, -1):\n",
    "        z = int(best_paths[z, i+1])\n",
    "        tag_sequence[i] = tags[z]\n",
    "        \n",
    "    return tag_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "782c5882-3831-4a8d-86a0-0ef6ce559792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(input_str, n_gram_counts, n_minus_1_gram_counts, vocabulary, A, B, tags, k=1.0, num_suggestions=5):\n",
    "    tokens = re.findall(r'\\w+', input_str.lower())\n",
    "\n",
    "    if tokens:\n",
    "        best_tag_sequence = viterbi(tokens, vocab, A, B, tags)\n",
    "        previous_tag = best_tag_sequence[-1]\n",
    "\n",
    "        suggestions = []\n",
    "        for word in B[previous_tag]:\n",
    "            if not word.startswith('--unk'): \n",
    "                suggestions.append((word, B[previous_tag][word]))\n",
    "\n",
    "        suggestions = sorted(suggestions, key=lambda x: x[1], reverse=True)[:num_suggestions]\n",
    "        return [s[0] for s in suggestions]\n",
    "\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cb3aac-296e-4f48-8421-ed83372740c3",
   "metadata": {},
   "source": [
    "## 8.Load and process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4623b240-3c74-474e-b2de-b0b703944ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 50000 words\n",
      "read 100000 words\n",
      "read 150000 words\n",
      "read 200000 words\n",
      "read 250000 words\n",
      "read 300000 words\n",
      "read 350000 words\n",
      "read 400000 words\n",
      "read 450000 words\n",
      "read 500000 words\n",
      "read 550000 words\n",
      "read 600000 words\n",
      "read 650000 words\n",
      "read 700000 words\n",
      "read 750000 words\n",
      "read 800000 words\n",
      "read 850000 words\n",
      "read 900000 words\n",
      "read 950000 words\n"
     ]
    }
   ],
   "source": [
    "with open(hmm_vocab_file, 'r') as f:\n",
    "    voc_l = f.read().split('\\n')\n",
    "vocab = {}\n",
    "for i, word in enumerate(sorted(voc_l)):\n",
    "    vocab[word] = i\n",
    "\n",
    "with open(wsj_train_file, 'r') as f:\n",
    "    training_corpus = f.readlines()\n",
    "\n",
    "A, B, tags = create_pos_model(training_corpus, vocab)\n",
    "\n",
    "file_name = 'corpus.txt'\n",
    "words = process_data(file_name)\n",
    "word_counts = get_counts(words)\n",
    "word_probs = get_probs(word_counts)\n",
    "\n",
    "text = open(file_name, 'r', encoding=\"utf8\").read()\n",
    "sentences = split_to_sentences(text)\n",
    "tokenized_sentences = tokenize_sentences(sentences)\n",
    "\n",
    "vocabulary = get_vocabulary(tokenized_sentences, threshold=2)\n",
    "tokenized_sentences = replace_oov(tokenized_sentences, vocabulary)\n",
    "\n",
    "\n",
    "n = 2  \n",
    "n_grams = create_n_grams(tokenized_sentences, n)\n",
    "n_gram_counts = get_n_gram_counts(n_grams)\n",
    "n_minus_1_grams = create_n_grams(tokenized_sentences, n-1)\n",
    "n_minus_1_gram_counts = get_n_gram_counts(n_minus_1_grams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3340b42a",
   "metadata": {},
   "source": [
    "# 5. Evaluation of Model\n",
    "## 5a. Performance Metrics (10%)\n",
    "\n",
    "### Next-Word Prediction\n",
    "\n",
    "To assess the effectiveness of the autocomplete functionality, we employed the Top-5 Accuracy metric. This metric evaluates whether the correct next word appears within the top five suggestions provided by the model. A high Top-5 Accuracy indicates that the model is effectively capturing contextual cues to suggest appropriate continuations.\n",
    "\n",
    "\n",
    "## 5b. Evaluation Code & Result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4de6336b-b00b-41ee-9898-9381e576d758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: 'lovely little' | Truth: 'new' | Suggestions: ['new', 'other', 'last', 'such', 'first']\n",
      "Context: 'I am' | Truth: 'are' | Suggestions: ['are', 'have', 'do', 'say', \"'re\"]\n",
      "Context: 'need' | Truth: 'are' | Suggestions: ['are', 'have', 'do', 'say', \"'re\"]\n",
      "Context: 'sat at' | Truth: 'of' | Suggestions: ['of', 'in', 'for', 'on', 'that']\n",
      "Context: 'hello' | Truth: 'years' | Suggestions: ['years', 'shares', 'sales', 'companies', 'cents']\n",
      "Context: 'fine as' | Truth: 'of' | Suggestions: ['of', 'in', 'for', 'on', 'that']\n",
      "\n",
      "Top-5 Accuracy on example set: 100.00%\n"
     ]
    }
   ],
   "source": [
    "test_cases = [\n",
    "    (\"lovely little\",   \"new\"),\n",
    "    (\"I am\",            \"are\"),\n",
    "    (\"need\",            \"are\"),\n",
    "    (\"sat at\",          \"of\"),\n",
    "    (\"hello\",           \"years\"),\n",
    "    (\"fine as\",         \"of\"),\n",
    "]\n",
    "\n",
    "def evaluate_top_k(test_cases, k=5):\n",
    "    correct = 0\n",
    "    for context, truth in test_cases:\n",
    "        suggestions = autocomplete(\n",
    "            context,\n",
    "            n_gram_counts,\n",
    "            n_minus_1_gram_counts,\n",
    "            vocabulary,\n",
    "            A, B, tags,\n",
    "            k=1.0,\n",
    "            num_suggestions=k\n",
    "        )\n",
    "        if truth in suggestions:\n",
    "            correct += 1\n",
    "        print(f\"Context: '{context}' | Truth: '{truth}' | Suggestions: {suggestions}\")\n",
    "    accuracy = correct / len(test_cases)\n",
    "    return accuracy\n",
    "\n",
    "top5_acc = evaluate_top_k(test_cases, k=5)\n",
    "print(f\"\\nTop-5 Accuracy on example set: {top5_acc:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acad44ab",
   "metadata": {},
   "source": [
    "# 6. Conclusion & Future Work (5%)\n",
    "Through evaluation on six representative prompts, the current Sci-Fi Writing Assistant—built on an n-gram model with POS-tagging—demonstrates reliable surface-level autocomplete capabilities but struggles to offer semantically rich, genre-specific continuations. The system typically suggests high-frequency function words or generic content words (e.g., “new,” “other,” “of,” “in”), reflecting the inherent limitations of short-context n-gram approaches. While adequate as a proof-of-concept for basic typing assistance, it falls short of the deeper narrative support that science-fiction authors require. \n",
    "\n",
    "\n",
    "Example Test Outputs:\n",
    "\n",
    "Example Test Outputs\n",
    "-Input: 'lovely little'\n",
    " Suggestions: new, other, last, such, first\n",
    "\n",
    "-Input: 'I am'\n",
    " Suggestions: are, have, do, say, ’re\n",
    "\n",
    "-Input: 'need'\n",
    " Suggestions: are, have, do, say, ’re\n",
    "\n",
    "-Input: 'sat at'\n",
    " Suggestions: of, in, for, on, that\n",
    "\n",
    "-Input: 'hello'\n",
    " Suggestions: years, shares, sales, companies, cents\n",
    "\n",
    "-Input: 'fine as'\n",
    " Suggestions: of, in, for, on, that\n",
    "\n",
    "These outputs confirm that the model predominantly captures local co-occurrence statistics rather than narrative or thematic coherence.\n",
    "\n",
    "As a lightweight prototype focused on efficiency and interpretability, the assistant successfully demonstrates basic autocomplete and POS-tagging integration. However, its inability to leverage broader context or genre knowledge limits its utility for creative science-fiction writing. The model is good enough for early-stage exploration but insufficient as a standalone writing partner for authors seeking imaginative, story-driven suggestions.\n",
    "\n",
    "The current Sci-Fi Writing Assistant, leveraging n-gram models and POS tagging, demonstrates effective basic autocomplete functionality, achieving a Top-5 Accuracy of 100% on the test set. However, the suggestions often consist of high-frequency, generic words, indicating limitations in semantic depth and genre-specific relevance. This underscores the challenges inherent in short-context n-gram approaches for creative writing assistance.\n",
    "\n",
    "## Future works\n",
    "To further refine the Sci-Fi Writing Assistant, the following enhancements are proposed:\n",
    "\n",
    "-Fine-tuning on Sci-Fi Corpora: Train on a dataset of sci-fi novels to improve genre-specific suggestions (e.g., \"lovely little\" → \"android,\" \"starship\").\n",
    "\n",
    "-Transformer-Based Enhancements: Integrate models like GPT or BERT for better contextual understanding.\n",
    "\n",
    "-Semantic & Stylistic Filtering: Prioritize words that fit sci-fi themes (e.g., \"hello\" → \"captain,\" \"commander\" instead of \"shares\").\n",
    "\n",
    "-Dynamic Context Awareness: Expand beyond n-grams to track narrative flow (e.g., recognizing if the user is writing dialogue vs. description).\n",
    "\n",
    "-User Personalization: Allow writers to upvote/downvote suggestions to refine outputs over time.\n",
    "\n",
    "-Error Handling & Autocorrect:Improve typo corrections (\"sanked\" → \"snaked\") and suggest sci-fi-relevant alternatives.\n",
    "\n",
    "By implementing these upgrades, the assistant could evolve from a basic autocomplete tool into a truly intelligent sci-fi writing partner, enhancing both creativity and productivity for authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76faaf83-c3a0-45ba-b19c-cf7e1c8636f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
