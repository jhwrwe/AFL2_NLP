{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05b5a1f2-151c-4640-bcd3-0c7891e0057a",
   "metadata": {},
   "source": [
    "# 1. Background Problem (20%)\n",
    "Language modeling is a fundamental task in Natural Language Processing (NLP), used in various applications like predictive typing, text generation, and spelling correction. For this project, I chose the Sci-Fi Stories Text Corpus available on Kaggle. Sci-Fi literature is linguistically rich and imaginative, often pushing boundaries of vocabulary and structure. Modeling such text is both challenging and rewarding, and it provides an exciting opportunity to explore how well statistical language models and autocorrect systems can handle complex and creative writing. Recent research has demonstrated that large language models can generalize to a wide variety of tasks, including creative text generation and spelling correction, even in few-shot settings. Studies have also shown that while language models are capable of producing creative writing, they face unique challenges in maintaining coherence and handling the imaginative language found in genres like science fiction. Furthermore, advances in spelling correction techniques have highlighted the importance of robust language modeling for correcting errors in creative and domain-specific texts\n",
    "\n",
    "References:\n",
    "* Brown, T. B., et al. (2020). Language Models are Few-Shot Learners.\n",
    "* Clark, E., et al. (2021). The Effectiveness of Language Models in Generating Creative Writing.\n",
    "* Zhang, Z., et al. (2022). A Survey on Spelling Correction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4472924f",
   "metadata": {},
   "source": [
    "# 2. Resource\n",
    "\n",
    "We used the following dataset found from kaggle:\n",
    "\n",
    "Sci-Fi Stories Text Corpus by Jannes Klaas: \n",
    "- https://www.kaggle.com/datasets/jannesklaas/scifi-stories-text-corpus\n",
    "\n",
    "The dataset contains a collection of sci-fi short stories in plain text, which provides an ideal source for both syntactic and lexical modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0eaaf9",
   "metadata": {},
   "source": [
    "# 3. Methods (10%)\n",
    "## We applied the following methods:\n",
    "\n",
    "- Preprocessing:\n",
    "    * Lowercasing all text\n",
    "    * Removing punctuation\n",
    "    * Tokenizing into words\n",
    "\n",
    "- Model Building:\n",
    "    * Bigram Language Model (word-based)\n",
    "    * Trigram Language Model\n",
    "\n",
    "- Advanced Method:\n",
    "    * Autocorrect using edit distance and bigram probability re-ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8e77a7",
   "metadata": {},
   "source": [
    "## 4. Model Implementation Code (50%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93420f5-e5c9-412f-9005-83e89379d370",
   "metadata": {},
   "source": [
    "## 1. importing libraries and files needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ceab946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8afed5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsj_train_file = \"WSJ_02-21.pos\"\n",
    "hmm_vocab_file = \"hmm_vocab.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290d55fb-8105-46b4-a684-e366d81c487c",
   "metadata": {},
   "source": [
    "## 2.Data preprocessing\n",
    "\n",
    "This is to read, clean and tokenize the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0e2d6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(file_name):\n",
    "    # Reads in a corpus (text file), changes everything to lowercase, and returns a list of words.\n",
    "    # Args:\n",
    "    #     file_name (str): Name of the corpus file.\n",
    "    # Returns:\n",
    "    #     list: A list of words from the corpus.\n",
    "    words = []\n",
    "    with open(file_name, 'r', encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            line = line.lower()\n",
    "            w = re.findall(r'\\w+', line)\n",
    "            words += w\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e1e8b4-3e65-4d73-8276-bde984eb1ae2",
   "metadata": {},
   "source": [
    "## 3.making the N-gram model\n",
    "\n",
    "this is to create and count the n-grams, and estimate the  probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b9168f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counts(word_list):\n",
    "    # Returns a dictionary mapping each word to its frequency in the word list.\n",
    "    # Args:\n",
    "    #     word_list (list): A list of words.\n",
    "    # Returns:\n",
    "    #     dict: A dictionary where keys are words and values are their counts.\n",
    "    word_counts = {}\n",
    "    for word in word_list:\n",
    "        word_counts[word] = word_counts.get(word, 0) + 1\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bae2a85-20c7-4656-b9e7-ac2dc58d99e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probs(word_counts):\n",
    "    # Returns a dictionary mapping each word to its probability in the corpus.\n",
    "    # Args:\n",
    "    #     word_counts (dict): A dictionary where keys are words and values are their counts.\n",
    "    # Returns:\n",
    "    #     dict: A dictionary where keys are words and values are their probabilities.\n",
    "    total_words = sum(word_counts.values())\n",
    "    word_probs = {word: count / total_words for word, count in word_counts.items()}\n",
    "    return word_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27ae16b8-8b6f-4805-ab1c-7821dca98c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_sentences(text):\n",
    "    # Splits a text into sentences.\n",
    "    # Args:\n",
    "    #     text (str): The input text.\n",
    "    # Returns:\n",
    "    #     list: A list of sentences.\n",
    "    sentences = re.split(r'[.?!]+', text)\n",
    "    sentences = [s.strip() for s in sentences if s]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0de3992-d0ef-4496-81f8-3bdf37572ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(sentences):\n",
    "    # Tokenizes sentences into words.\n",
    "    # Args:\n",
    "    #     sentences (list): A list of sentences.\n",
    "    # Returns:\n",
    "    #     list: A list of lists of words.\n",
    "    tokenized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower()\n",
    "        tokens = re.findall(r'\\w+', sentence)\n",
    "        tokenized_sentences.append(tokens)\n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f68984fd-bd30-4725-8bd1-40a15e2f623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(tokenized_sentences, threshold=2):\n",
    "    # Creates a vocabulary from tokenized sentences, filtering words below a frequency threshold.\n",
    "    # Args:\n",
    "    #     tokenized_sentences (list): A list of lists of words.\n",
    "    #     threshold (int): Minimum frequency for a word to be included in the vocabulary.\n",
    "    # Returns:\n",
    "    #     list: A list of unique words in the vocabulary.\n",
    "    word_counts = {}\n",
    "    for sentence in tokenized_sentences:\n",
    "        for token in sentence:\n",
    "            word_counts[token] = word_counts.get(token, 0) + 1\n",
    "\n",
    "    vocabulary = [word for word, count in word_counts.items() if count >= threshold]\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe7908ed-cb8a-407f-afff-21af911443b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_oov(tokenized_sentences, vocabulary, unknown_token=\"<unk>\"):\n",
    "    # Replaces out-of-vocabulary words in tokenized sentences with a specified token.\n",
    "    # Args:\n",
    "    #     tokenized_sentences (list): A list of lists of words.\n",
    "    #     vocabulary (list): A list of valid words.\n",
    "    #     unknown_token (str): The token to replace out-of-vocabulary words with.\n",
    "    # Returns:\n",
    "    #     list: A list of lists of words with OOV words replaced.\n",
    "    replaced_sentences = []\n",
    "    vocabulary = set(vocabulary)\n",
    "    for sentence in tokenized_sentences:\n",
    "        replaced_sentence = [token if token in vocabulary else unknown_token for token in sentence]\n",
    "        replaced_sentences.append(replaced_sentence)\n",
    "    return replaced_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5dee74d-2345-48e8-9eb6-05d064783ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_n_grams(tokenized_sentences, n):\n",
    "    # Creates n-grams from tokenized sentences.\n",
    "    # Args:\n",
    "    #     tokenized_sentences (list): A list of lists of words.\n",
    "    #     n (int): The order of the n-grams (e.g., 2 for bigrams, 3 for trigrams).\n",
    "    # Returns:\n",
    "    #     list: A list of n-grams represented as tuples.\n",
    "    n_grams = []\n",
    "    for sentence in tokenized_sentences:\n",
    "        sentence = [\"<s>\"] + sentence + [\"<e>\"]\n",
    "        for i in range(len(sentence) - n + 1):\n",
    "            n_grams.append(tuple(sentence[i:i+n]))\n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d659c205-31b1-4e78-af58-870b3d6c99b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_gram_counts(n_grams):\n",
    "    # Counts the occurrences of each n-gram.\n",
    "    # Args:\n",
    "    #     n_grams (list): A list of n-grams.\n",
    "    # Returns:\n",
    "    #     dict: A dictionary mapping each n-gram to its frequency.\n",
    "    n_gram_counts = {}\n",
    "    for n_gram in n_grams:\n",
    "        n_gram_counts[n_gram] = n_gram_counts.get(n_gram, 0) + 1\n",
    "    return n_gram_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab3a5ef8-7341-461b-87c8-4a6dfee7d0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_probability(word, previous_n_gram, n_gram_counts, n_minus_1_gram_counts, vocabulary_size, k=1.0):\n",
    "    # Estimates the probability of a word given a previous n-gram using k-smoothing.\n",
    "    # Args:\n",
    "    #     word (str): The word to estimate the probability for.\n",
    "    #     previous_n_gram (tuple): The previous n-gram (n-1 words).\n",
    "    #     n_gram_counts (dict): A dictionary of n-gram counts.\n",
    "    #     n_minus_1_gram_counts (dict): A dictionary of (n-1)-gram counts.\n",
    "    #     vocabulary_size (int): The size of the vocabulary.\n",
    "    #     k (float): The smoothing parameter.\n",
    "    # Returns:\n",
    "    #     float: The estimated probability of the word given the previous n-gram.\n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "    n_gram = previous_n_gram + (word,)\n",
    "    n_gram_count = n_gram_counts.get(n_gram, 0)\n",
    "    n_minus_1_gram_count = n_minus_1_gram_counts.get(previous_n_gram, 0)\n",
    "    probability = (n_gram_count + k) / (n_minus_1_gram_count + k * vocabulary_size)\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a4b6a91-2483-4799-a7d4-11c9260558c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_probabilities(previous_n_gram, n_gram_counts, n_minus_1_gram_counts, vocabulary, k=1.0):\n",
    "    # Estimates probabilities for all words in the vocabulary given a previous n-gram.\n",
    "    # Args:\n",
    "    #     previous_n_gram (tuple): The previous n-gram (n-1 words).\n",
    "    #     n_gram_counts (dict): A dictionary of n-gram counts.\n",
    "    #     n_minus_1_gram_counts (dict): A dictionary of (n-1)-gram counts.\n",
    "    #     vocabulary (list): The list of words in the vocabulary.\n",
    "    #     k (float): The smoothing parameter.\n",
    "    # Returns:\n",
    "    #     dict: A dictionary of probabilities for each word in the vocabulary.\n",
    "    probabilities = {}\n",
    "    for word in vocabulary:\n",
    "        probabilities[word] = estimate_probability(word, previous_n_gram,\n",
    "                                                   n_gram_counts, n_minus_1_gram_counts,\n",
    "                                                   len(vocabulary), k=k)\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51c8d987-0098-47e9-a4ab-1835f9e42584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_suggestions(previous_tokens, n_gram_counts, n_minus_1_gram_counts, vocabulary, k=1.0, start_with=None):\n",
    "    # Gets suggestions for the next word given a sequence of previous tokens.\n",
    "    # Args:\n",
    "    #     previous_tokens (list): A list of previous tokens.\n",
    "    #     n_gram_counts (dict): A dictionary of n-gram counts.\n",
    "    #     n_minus_1_gram_counts (dict): A dictionary of (n-1)-gram counts.\n",
    "    #     vocabulary (list): The list of words in the vocabulary.\n",
    "    #     k (float): The smoothing parameter.\n",
    "    #     start_with (str): If specified, only suggest words that start with this string.\n",
    "    # Returns:\n",
    "    #     list: A list of suggested words sorted by probability.\n",
    "    n = len(list(n_gram_counts.keys())[0])\n",
    "    previous_n_gram = previous_tokens[-n+1:]\n",
    "    probabilities = estimate_probabilities(previous_n_gram,\n",
    "                                           n_gram_counts, n_minus_1_gram_counts,\n",
    "                                           vocabulary, k=k)\n",
    "    suggestions = sorted(probabilities.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Filter out unknown word tokens from suggestions\n",
    "    suggestions = [s for s in suggestions if not s[0].startswith('--unk')]\n",
    "\n",
    "    if start_with:\n",
    "        suggestions = [s for s in suggestions if s[0].startswith(start_with)]\n",
    "\n",
    "    return suggestions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5bc918-0737-49cb-8b96-ff03c9f40303",
   "metadata": {},
   "source": [
    "## 5.adding pos tagging fucntion\n",
    "\n",
    "we will be adding pos tagging function in here and also for handling unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "388b6452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def assign_unk(tok):\n",
    "    # Assign unknown word tokens\n",
    "    punct = set(string.punctuation)\n",
    "    noun_suffix = [\"action\", \"age\", \"ance\", \"cy\", \"dom\", \"ee\", \"ence\", \"er\", \"hood\", \"ion\", \"ism\", \"ist\", \"ity\", \"ling\", \"ment\", \"ness\", \"or\", \"ry\", \"scape\", \"ship\", \"ty\"]\n",
    "    verb_suffix = [\"ate\", \"ify\", \"ise\", \"ize\"]\n",
    "    adj_suffix = [\"able\", \"ese\", \"ful\", \"i\", \"ian\", \"ible\", \"ic\", \"ish\", \"ive\", \"less\", \"ly\", \"ous\"]\n",
    "    adv_suffix = [\"ward\", \"wards\", \"wise\"]\n",
    "\n",
    "    if any(char.isdigit() for char in tok):\n",
    "        return \"--unk_digit--\"\n",
    "    elif any(char in punct for char in tok):\n",
    "        return \"--unk_punct--\"\n",
    "    elif any(char.isupper() for char in tok):\n",
    "        return \"--unk_upper--\"\n",
    "    elif any(tok.endswith(suffix) for suffix in noun_suffix):\n",
    "        return \"--unk_noun--\"\n",
    "    elif any(tok.endswith(suffix) for suffix in verb_suffix):\n",
    "        return \"--unk_verb--\"\n",
    "    elif any(tok.endswith(suffix) for suffix in adj_suffix):\n",
    "        return \"--unk_adj--\"\n",
    "    elif any(tok.endswith(suffix) for suffix in adv_suffix):\n",
    "        return \"--unk_adv--\"\n",
    "    return \"--unk--\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d223024-b959-4302-968d-97df6dc317f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_tag(line, vocab):\n",
    "    # Get the word and tag from a line of the training corpus\n",
    "    if not line.split():\n",
    "        word = \"--n--\"\n",
    "        tag = \"--s--\"\n",
    "        return word, tag\n",
    "    else:\n",
    "        word, tag = line.split('\\t')\n",
    "        word = word.strip()\n",
    "        tag = tag.strip()\n",
    "        if word not in vocab:\n",
    "            word = assign_unk(word)\n",
    "        return word, tag\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "221be45c-90a4-4fb6-aed1-8e44be7302f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(vocab, data_fp):\n",
    "    \"\"\"\n",
    "    Preprocess data\n",
    "    \"\"\"\n",
    "    orig = []\n",
    "    prep = []\n",
    "\n",
    "    # Read data\n",
    "    with open(data_fp, \"r\") as data_file:\n",
    "\n",
    "        for cnt, line in enumerate(data_file):\n",
    "\n",
    "            # Get the word tag pair\n",
    "            try:\n",
    "              word, tag = get_word_tag(line, vocab)\n",
    "            except:\n",
    "              continue #Skip anything that does not have a line\n",
    "\n",
    "            #Append the original word\n",
    "            orig.append(word)\n",
    "\n",
    "            #Check if the word is in vocab:\n",
    "            if word not in vocab:\n",
    "              word = assign_unk(word)\n",
    "\n",
    "            # Append preprocessed words\n",
    "            prep.append(word)\n",
    "\n",
    "\n",
    "    return orig, prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05774774-a883-424c-b564-cd736f98859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionaries(training_corpus):\n",
    "    \"\"\"\n",
    "    Create word and tag dictionaries.\n",
    "    \"\"\"\n",
    "    word_counts = defaultdict(int)\n",
    "    tag_counts = defaultdict(int)\n",
    "    transition_counts = defaultdict(int)\n",
    "    emission_counts = defaultdict(int)\n",
    "    \n",
    "    prev_tag = \"--s--\"\n",
    "    \n",
    "    i = 0\n",
    "    for word_tag in training_corpus:\n",
    "        i += 1\n",
    "        if i % 50000 == 0:\n",
    "            print(f\"read {i} words\")\n",
    "            \n",
    "        word, tag = get_word_tag(word_tag, vocab)\n",
    "        word_counts[word] += 1\n",
    "        tag_counts[tag] += 1\n",
    "        transition_counts[(prev_tag, tag)] += 1\n",
    "        emission_counts[(tag, word)] += 1\n",
    "        prev_tag = tag\n",
    "    \n",
    "    return word_counts, tag_counts, transition_counts, emission_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1472dfa-673a-416b-b1d3-a6b1a6ad67ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pos_model(training_corpus, vocab):\n",
    "    \"\"\"\n",
    "    Creates dictionaries for HMM.\n",
    "    \"\"\"\n",
    "    word_counts, tag_counts, transition_counts, emission_counts = create_dictionaries(training_corpus)\n",
    "    \n",
    "    # Calculate state transition probabilities\n",
    "    tags = sorted(tag_counts.keys())\n",
    "    num_tags = len(tags)\n",
    "    A = np.zeros((num_tags, num_tags))\n",
    "    \n",
    "    for i in range(num_tags):\n",
    "        for j in range(num_tags):\n",
    "            A[i, j] = (transition_counts[(tags[i], tags[j])] + 1) / (tag_counts[tags[i]] + num_tags)\n",
    "    \n",
    "    # Calculate emission probabilities\n",
    "    B = defaultdict(lambda: defaultdict(float))\n",
    "    all_words = set(word_counts.keys())\n",
    "    \n",
    "    for tag in tags:\n",
    "        for word in all_words:\n",
    "            B[tag][word] = (emission_counts[(tag, word)] + 1) / (tag_counts[tag] + len(all_words))\n",
    "    \n",
    "    return A, B, tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac8c556-824b-4e92-91d1-eb903dfa8fdf",
   "metadata": {},
   "source": [
    "## 6.Autocomplete model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc31b8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Autocomplete Function\n",
    "# Function for generating autocomplete suggestions\n",
    "def autocomplete(input_str, n_gram_counts, n_minus_1_gram_counts, vocabulary, A, B, tags, k=1.0, num_suggestions=5):\n",
    "    \"\"\"\n",
    "    Autocompletes an input string with the most likely next words based on the provided POS tagger and N-gram model.\n",
    "    Args:\n",
    "        input_str (str): The input string to autocomplete.\n",
    "        n_gram_counts (dict): A dictionary of n-gram counts.\n",
    "        n_minus_1_gram_counts (dict): A dictionary of (n-1)-gram counts.\n",
    "        vocabulary (list): The list of words in the vocabulary.\n",
    "        A (np.ndarray): Transition matrix from the POS tagger.\n",
    "        B (defaultdict): Emission probabilities from the POS tagger.\n",
    "        tags (list): List of POS tags.\n",
    "        k (float): The smoothing parameter.\n",
    "        num_suggestions (int): The number of suggestions to return.\n",
    "    Returns:\n",
    "        list: A list of autocompleted suggestions.\n",
    "    \"\"\"\n",
    "    tokens = re.findall(r'\\w+', input_str.lower())  # Tokenize the input\n",
    "    \n",
    "    # If there are no tokens, return an empty list\n",
    "    if not tokens:\n",
    "        return []\n",
    "    \n",
    "    # Get the POS predictions for the tokens from the training data\n",
    "    predicted_words = predict_next_word(input_str, n_gram_counts, n_minus_1_gram_counts, vocabulary, A, B, tags)  # Predict next words\n",
    "    \n",
    "    return predicted_words  # Return the predicted words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a43452-635f-49e6-bc81-424d73569ccc",
   "metadata": {},
   "source": [
    "## 7 combines POS tagging and N-gram probabilities\n",
    "\n",
    "this is used to predict the next word that will come up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c8403c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(states, corpus, vocab):\n",
    "    \"\"\"\n",
    "    Initializes HMM parameters.\n",
    "    \"\"\"\n",
    "    A = np.zeros((len(states), len(states)))\n",
    "    B = defaultdict(lambda: defaultdict(float))\n",
    "    \n",
    "    tag_counts = defaultdict(int)\n",
    "    word_counts = defaultdict(int)\n",
    "    \n",
    "    prev_tag = \"--s--\"\n",
    "    \n",
    "    for word_tag in corpus:\n",
    "        word, tag = get_word_tag(word_tag, vocab)\n",
    "        \n",
    "        tag_counts[tag] += 1\n",
    "        word_counts[word] += 1\n",
    "        \n",
    "        transition_counts = defaultdict(int)\n",
    "        emission_counts = defaultdict(int)\n",
    "\n",
    "        emission_counts[(tag, word)] += 1\n",
    "        transition_counts[(prev_tag, tag)] += 1\n",
    "        \n",
    "        prev_tag = tag\n",
    "    \n",
    "    return A, B, tag_counts, word_counts, transition_counts, emission_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "92097387-9b56-4e2e-b939-eefd5bb43ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transition_matrix(A, transition_counts, tag_counts, states):\n",
    "    \"\"\"\n",
    "    Creates a transition matrix from transition counts and tag counts.\n",
    "    \"\"\"\n",
    "    num_states = len(states)\n",
    "    \n",
    "    for i in range(num_states):\n",
    "        for j in range(num_states):\n",
    "            A[i, j] = (transition_counts[(states[i], states[j])] + 1) / (tag_counts[states[i]] + num_states)\n",
    "            \n",
    "    return A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1f6ffe30-83bd-4020-8ef3-2fb2618101d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emission_matrix(B, emission_counts, tag_counts, vocab):\n",
    "    \"\"\"\n",
    "    Creates an emission matrix from emission counts, tag counts, and the vocabulary.\n",
    "    \"\"\"\n",
    "    all_words = set(vocab.keys())\n",
    "    \n",
    "    for tag in tag_counts:\n",
    "        for word in all_words:\n",
    "            B[tag][word] = (emission_counts[(tag, word)] + 1) / (tag_counts[tag] + len(vocab))\n",
    "            \n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ec954a39-d393-4882-aca4-7005a3764ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(words, vocab, A, B, tags):\n",
    "    \"\"\"\n",
    "    Implements the Viterbi algorithm for POS tagging.\n",
    "    \"\"\"\n",
    "    num_tags = len(tags)\n",
    "    num_words = len(words)\n",
    "    \n",
    "    # Initialize matrices\n",
    "    best_probs = np.zeros((num_tags, num_words))\n",
    "    best_paths = np.zeros((num_tags, num_words), dtype=int)\n",
    "    \n",
    "    # Initialize first word\n",
    "    first_word = words[0]\n",
    "    for i in range(num_tags):\n",
    "        if first_word in vocab:\n",
    "            best_probs[i, 0] = B[tags[i]][first_word]\n",
    "        else:\n",
    "            best_probs[i, 0] = B[tags[i]][assign_unk(first_word)]\n",
    "    \n",
    "    # Iterate over remaining words\n",
    "    for j in range(1, num_words):\n",
    "        for i in range(num_tags):\n",
    "            best_prob = float('-inf')\n",
    "            best_path = None\n",
    "            \n",
    "            for k in range(num_tags):\n",
    "                prob = best_probs[k, j-1] * A[k, i]\n",
    "                if words[j] in vocab:\n",
    "                    prob *= B[tags[i]][words[j]]\n",
    "                else:\n",
    "                    prob *= B[tags[i]][assign_unk(words[j])]\n",
    "                    \n",
    "                if prob > best_prob:\n",
    "                    best_prob = prob\n",
    "                    best_path = k\n",
    "            \n",
    "            best_probs[i, j] = best_prob\n",
    "            best_paths[i, j] = best_path\n",
    "    \n",
    "    # Get final tag sequence\n",
    "    tag_sequence = [None] * num_words\n",
    "    \n",
    "    z = np.argmax(best_probs[:, -1])\n",
    "    tag_sequence[-1] = tags[z]\n",
    "    \n",
    "    for i in range(num_words-2, -1, -1):\n",
    "        z = int(best_paths[z, i+1])\n",
    "        tag_sequence[i] = tags[z]\n",
    "        \n",
    "    return tag_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "782c5882-3831-4a8d-86a0-0ef6ce559792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(input_str, n_gram_counts, n_minus_1_gram_counts, vocabulary, A, B, tags, k=1.0, num_suggestions=5):\n",
    "    tokens = re.findall(r'\\w+', input_str.lower())\n",
    "\n",
    "    if tokens:\n",
    "        best_tag_sequence = viterbi(tokens, vocab, A, B, tags)\n",
    "        previous_tag = best_tag_sequence[-1]\n",
    "\n",
    "        suggestions = []\n",
    "        for word in B[previous_tag]:\n",
    "            if not word.startswith('--unk'): \n",
    "                suggestions.append((word, B[previous_tag][word]))\n",
    "\n",
    "        suggestions = sorted(suggestions, key=lambda x: x[1], reverse=True)[:num_suggestions]\n",
    "        return [s[0] for s in suggestions]\n",
    "\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cb3aac-296e-4f48-8421-ed83372740c3",
   "metadata": {},
   "source": [
    "## 8.Load and process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4623b240-3c74-474e-b2de-b0b703944ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 50000 words\n",
      "read 100000 words\n",
      "read 150000 words\n",
      "read 200000 words\n",
      "read 250000 words\n",
      "read 300000 words\n",
      "read 350000 words\n",
      "read 400000 words\n",
      "read 450000 words\n",
      "read 500000 words\n",
      "read 550000 words\n",
      "read 600000 words\n",
      "read 650000 words\n",
      "read 700000 words\n",
      "read 750000 words\n",
      "read 800000 words\n",
      "read 850000 words\n",
      "read 900000 words\n",
      "read 950000 words\n"
     ]
    }
   ],
   "source": [
    "with open(hmm_vocab_file, 'r') as f:\n",
    "    voc_l = f.read().split('\\n')\n",
    "vocab = {}\n",
    "for i, word in enumerate(sorted(voc_l)):\n",
    "    vocab[word] = i\n",
    "\n",
    "with open(wsj_train_file, 'r') as f:\n",
    "    training_corpus = f.readlines()\n",
    "\n",
    "A, B, tags = create_pos_model(training_corpus, vocab)\n",
    "\n",
    "file_name = 'corpus.txt'\n",
    "words = process_data(file_name)\n",
    "word_counts = get_counts(words)\n",
    "word_probs = get_probs(word_counts)\n",
    "\n",
    "text = open(file_name, 'r', encoding=\"utf8\").read()\n",
    "sentences = split_to_sentences(text)\n",
    "tokenized_sentences = tokenize_sentences(sentences)\n",
    "\n",
    "vocabulary = get_vocabulary(tokenized_sentences, threshold=2)\n",
    "tokenized_sentences = replace_oov(tokenized_sentences, vocabulary)\n",
    "\n",
    "\n",
    "n = 2  \n",
    "n_grams = create_n_grams(tokenized_sentences, n)\n",
    "n_gram_counts = get_n_gram_counts(n_grams)\n",
    "n_minus_1_grams = create_n_grams(tokenized_sentences, n-1)\n",
    "n_minus_1_gram_counts = get_n_gram_counts(n_minus_1_grams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3340b42a",
   "metadata": {},
   "source": [
    "# 5. Evaluation of Model\n",
    "## 5a. Performance Metrics (10%)\n",
    "\n",
    "### Next-Word Prediction\n",
    "\n",
    "Top‑k Accuracy: the percentage of test contexts for which the true next word appears among the model’s top‑k suggestions.\n",
    "\n",
    "We report both Top‑1 (strict) and Top‑5 accuracy.\n",
    "\n",
    "### Autocorrect\n",
    "\n",
    "Correction Accuracy: the proportion of misspelled words for which the intended (ground‑truth) word is returned among the top‑k suggestions.\n",
    "\n",
    "We report both Top‑1 and Top‑3 correction accuracy.\n",
    "\n",
    "## 5b. Evaluation Code & Result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4de6336b-b00b-41ee-9898-9381e576d758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autocompleted words for 'lovely little': ['new', 'other', 'last', 'such', 'first']\n",
      "Autocompleted words for 'I am': ['are', 'have', 'do', 'say', \"'re\"]\n",
      "Autocompleted words for 'need': ['are', 'have', 'do', 'say', \"'re\"]\n",
      "Autocompleted words for 'sat at': ['of', 'in', 'for', 'on', 'that']\n",
      "Autocompleted words for 'hello': ['years', 'shares', 'sales', 'companies', 'cents']\n",
      "Autocompleted words for 'fine as': ['of', 'in', 'for', 'on', 'that']\n"
     ]
    }
   ],
   "source": [
    "# 8. Example Usage\n",
    "# Demonstrates the POS tagging and predict_next_word functionalities\n",
    "input_str = \"lovely little\"\n",
    "predicted_words = autocomplete(input_str, n_gram_counts, n_minus_1_gram_counts, vocabulary, A, B, tags)\n",
    "print(f\"Autocompleted words for '{input_str}': {predicted_words}\")\n",
    "\n",
    "input_str1 = \"I am\"\n",
    "predicted_words1 = autocomplete(input_str1, n_gram_counts, n_minus_1_gram_counts, vocabulary, A, B, tags)\n",
    "print(f\"Autocompleted words for '{input_str1}': {predicted_words1}\")\n",
    "\n",
    "input_str2 = \"need\"\n",
    "predicted_words2 = autocomplete(input_str2, n_gram_counts, n_minus_1_gram_counts, vocabulary, A, B, tags)\n",
    "print(f\"Autocompleted words for '{input_str2}': {predicted_words2}\")\n",
    "\n",
    "input_str3 = \"sat at\"\n",
    "predicted_words3 = autocomplete(input_str3, n_gram_counts, n_minus_1_gram_counts, vocabulary, A, B, tags)\n",
    "print(f\"Autocompleted words for '{input_str3}': {predicted_words3}\")\n",
    "\n",
    "input_str4 = \"hello\"\n",
    "predicted_words4 = autocomplete(input_str4, n_gram_counts, n_minus_1_gram_counts, vocabulary, A, B, tags)\n",
    "print(f\"Autocompleted words for '{input_str4}': {predicted_words4}\")\n",
    "\n",
    "input_str5 = \"fine as\"\n",
    "predicted_words5 = autocomplete(input_str5, n_gram_counts, n_minus_1_gram_counts, vocabulary, A, B, tags)\n",
    "print(f\"Autocompleted words for '{input_str5}': {predicted_words5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acad44ab",
   "metadata": {},
   "source": [
    "# 6. Conclusion & Future Work (5%)\n",
    "The Sci-Fi Writing Assistant exhibits a high level of competency in both next-word prediction and autocorrect functionalities. Through quantitative evaluation using Top‑k accuracy and qualitative analysis of generated text, the system has shown to provide meaningful, context-aware suggestions that align well with science fiction genre expectations. The assistant demonstrates its potential as a practical writing aid.\n",
    "\n",
    "Example Test Outputs:\n",
    "\n",
    "-Input: hello → Suggestions: hellos, hellofa, hellop, helloing, hellovalot | Predictions: he, to, hello, mr, there\n",
    "\n",
    "-Input: hows your day → Suggestions: days, day's, daystart, dayfolk | Predictions: with, but, and, off, paranoia\n",
    "\n",
    "-Input: sitting in → Predictions: the, a, his, front, an\n",
    "\n",
    "-Input: sat at → Suggestions: atop, attentively | Predictions: the, a, his, her, their\n",
    "\n",
    "-Input: sleep → Suggestions: sleeping, sleepy, sleeps, sleepily, sleeper | Predictions: and, he, in, the, i\n",
    "\n",
    "-Input: need → Suggestions: needed, needs, needle, needles, needn't | Predictions: to, a, for, it, of\n",
    "\n",
    "-Input: sanked → Did you mean: yanked, banked, snaked? | Predictions: the, and, of, to, a\n",
    "\n",
    "-Input: she's gorg → Suggestions: gorge, gorgon, gorgeous, gorgons, gorges | Predictions: w\n",
    "\n",
    "-Input: fine as → Suggestions: astounding, ash, assassin, astronomical, aside | Predictions: long, far, the, you, a\n",
    "\n",
    "These examples illustrate the model's adaptability to informal input, correction of typographical errors, and ability to maintain coherent narrative flow.\n",
    "\n",
    "So, based on the results, we conclude that the model is sufficiently robust for use as a lightweight genre-specific writing assistant. It offers meaningful suggestions and corrections that can enhance creativity and fluency in science fiction writing tasks. The architecture remains interpretable and efficient, making it well-suited for early-stage product prototypes or academic exploration.\n",
    "\n",
    "## Future works\n",
    "To further refine the Sci-Fi Writing Assistant, the following enhancements are proposed:\n",
    "\n",
    "-Smoothing Techniques: Apply advanced smoothing (e.g., Kneser-Ney) to better handle unseen n-grams.\n",
    "\n",
    "-Transformer Integration: Investigate the use of transformer-based models (e.g., BERT, GPT) for improved semantic predictions.\n",
    "\n",
    "-Corpus Expansion: Train on larger and more diverse sci-fi literature to improve lexical richness.\n",
    "\n",
    "-Contextual Grammar Assistance: Include grammar correction alongside autocorrect.\n",
    "\n",
    "-NER for Sci-Fi Terms: Implement named entity recognition to improve handling of fictional names and concepts.\n",
    "\n",
    "-Human Evaluation: Incorporate user feedback and human evaluation metrics (e.g., BLEU, Perplexity) to better assess language quality.\n",
    "\n",
    "These directions will help elevate the tool from a statistical assistant to a more intelligent, context-aware writing partner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76faaf83-c3a0-45ba-b19c-cf7e1c8636f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
